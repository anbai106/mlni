{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "protein_path = \"dataset/imputed_CruchagaLab_CSF.csv\" # Using the imputed dataset\n",
    "protein_metadata_path = \"dataset/ADNI_Cruchaga_lab_CSF.csv\"\n",
    "imaging_path = \"dataset/MuSIC_cov.tsv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered cardiovascular dataset saved as 'dataset/ADNI_cardiovascular.csv'\n",
      "Filtered brain dataset saved as 'dataset/ADNI_brain.csv'\n",
      "Filtered digestive dataset saved as 'dataset/ADNI_digestive.csv'\n",
      "Filtered endocrine dataset saved as 'dataset/ADNI_endocrine.csv'\n",
      "Filtered female_reproductive dataset saved as 'dataset/ADNI_female_reproductive.csv'\n",
      "Filtered hepatic dataset saved as 'dataset/ADNI_hepatic.csv'\n",
      "Filtered kidney dataset saved as 'dataset/ADNI_kidney.csv'\n",
      "Filtered male_reproductive dataset saved as 'dataset/ADNI_male_reproductive.csv'\n",
      "Filtered pulmonary dataset saved as 'dataset/ADNI_pulmonary.csv'\n",
      "Filtered retina dataset saved as 'dataset/ADNI_retina.csv'\n",
      "Filtered skin dataset saved as 'dataset/ADNI_skin.csv'\n"
     ]
    }
   ],
   "source": [
    "# Paths for different body system protein files\n",
    "body_system_proteins_paths = {\n",
    "    \"cardiovascular\": \"dataset/matched/matched_cardiovascular.csv\",\n",
    "    \"brain\": \"dataset/matched/matched_brain_tissue.csv\",\n",
    "    \"digestive\": \"dataset/matched/matched_digestive_system.csv\",\n",
    "    \"endocrine\": \"dataset/matched/matched_endocrine.csv\",\n",
    "    \"female_reproductive\": \"dataset/matched/matched_female_reproductive.csv\",\n",
    "    \"hepatic\": \"dataset/matched/matched_hepatic.csv\",\n",
    "    \"kidney\": \"dataset/matched/matched_kidney.csv\",\n",
    "    \"male_reproductive\": \"dataset/matched/matched_male_reproductive.csv\",\n",
    "    \"pulmonary\": \"dataset/matched/matched_pulmonary.csv\",\n",
    "    \"retina\": \"dataset/matched/matched_retina.csv\",\n",
    "    \"skin\": \"dataset/matched/matched_skin.csv\",\n",
    "}\n",
    "\n",
    "protein_metadata = pd.read_csv(protein_metadata_path)\n",
    "\n",
    "def filter_and_save_metadata(body_system_name, proteins_path):\n",
    "    \"\"\"\n",
    "    Filters the protein metadata for a specific body system\n",
    "    and saves the result as a CSV file.\n",
    "    \"\"\"\n",
    "    # Load the specific body system proteins\n",
    "    body_system_proteins = pd.read_csv(proteins_path)\n",
    "    \n",
    "    # Get unique genes from 'MatchedGene'\n",
    "    body_system_genes = body_system_proteins[\"MatchedGene\"].unique()\n",
    "    \n",
    "    # Filter rows in protein_metadata where 'EntrezGeneSymbol' matches body system genes\n",
    "    filtered_metadata = protein_metadata[protein_metadata[\"EntrezGeneSymbol\"].isin(body_system_genes)]\n",
    "    \n",
    "    # Drop duplicates based on 'EntrezGeneSymbol'\n",
    "    filtered_metadata = filtered_metadata.drop_duplicates(subset=\"EntrezGeneSymbol\")\n",
    "    \n",
    "    # Save the filtered dataset with an appropriate name\n",
    "    output_path = f\"dataset/ADNI_{body_system_name}.csv\"\n",
    "    filtered_metadata.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Filtered {body_system_name} dataset saved as '{output_path}'\")\n",
    "\n",
    "\n",
    "# Loop through each body system and process the files\n",
    "for body_system_name, proteins_path in body_system_proteins_paths.items():\n",
    "    filter_and_save_metadata(body_system_name, proteins_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the datasets\n",
    "protein = pd.read_csv(protein_path)\n",
    "imaging_data = pd.read_csv(imaging_path, sep=\"\\t\")\n",
    "\n",
    "# Extract the last 4 digits from participant_id in the imaging dataset\n",
    "imaging_data[\"RID_from_participant_id\"] = imaging_data[\"participant_id\"].str[-4:].astype(int)\n",
    "\n",
    "# Merge the datasets on RID\n",
    "merged_data = pd.merge(protein, imaging_data, left_on=\"RID\", right_on=\"RID_from_participant_id\", how=\"inner\")\n",
    "\n",
    "# # Identify unmatched row(s)\n",
    "# merged_with_left_join = pd.merge(protein, imaging_data, left_on=\"RID\", right_on=\"RID_from_participant_id\", how=\"left\")\n",
    "# unmatched_row = merged_with_left_join[merged_with_left_join[\"RID_from_participant_id\"].isna()]\n",
    "\n",
    "# # Print the unmatched row\n",
    "# print(\"Unmatched Row:\")\n",
    "# print(unmatched_row)\n",
    "\n",
    "# Drop the intermediate RID column from the imaging dataset if not needed\n",
    "merged_data.drop(columns=[\"RID_from_participant_id\"], inplace=True)\n",
    "\n",
    "# Select required columns from imaging_path and protein_path\n",
    "# Columns from imaging_path\n",
    "metadata_columns = [\"participant_id\", \"session_id\", \"age\", \"sex\", \"diagnosis\"]\n",
    "\n",
    "# Protein concentration columns from protein_path\n",
    "protein_columns = [col for col in protein.columns if col.startswith(\"X\")]  # Assuming concentrations start with \"X\"\n",
    "\n",
    "# Combine the required columns\n",
    "required_columns = metadata_columns + protein_columns\n",
    "filtered_merged_data = merged_data[required_columns]\n",
    "\n",
    "\n",
    "# Step 5: Save the filtered merged dataset\n",
    "filtered_merged_data.to_csv(\"dataset/merged_dataset.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_male_reproductive.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_female_reproductive.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_skin.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_digestive.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_hepatic.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_pulmonary.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_brain.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_kidney.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_retina.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_cardiovascular.tsv'\n",
      "Filtered dataset saved as 'dataset/final/filtered_ADNI_endocrine.tsv'\n",
      "Processing complete for all ADNI datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
      "/var/folders/sl/d6nmsrq10rq3p3h9kkc5r00h0000gn/T/ipykernel_49678/3516994937.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Paths to datasets\n",
    "filtered_merged_path = \"dataset/merged_dataset.csv\"\n",
    "\n",
    "# Directory containing all ADNI CSV files\n",
    "adni_files_directory = \"dataset/ADNI_matched\"\n",
    "output_directory = \"dataset/final_datasets/\" # Save to the final folder\n",
    "\n",
    "# Load the merged dataset\n",
    "filtered_merged_data = pd.read_csv(filtered_merged_path)\n",
    "\n",
    "# Get a list of all ADNI CSV files in the directory\n",
    "adni_files = [f for f in os.listdir(adni_files_directory) if f.startswith(\"ADNI_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through each ADNI file and process it\n",
    "for adni_file in adni_files:\n",
    "    # Load the ADNI data\n",
    "    adni_data_path = os.path.join(adni_files_directory, adni_file)\n",
    "    adni_data = pd.read_csv(adni_data_path)\n",
    "    \n",
    "    # Extract the list of analytes for the current body system\n",
    "    analytes = adni_data[\"Analytes\"].unique()\n",
    "    \n",
    "    # Identify protein columns in filtered_merged_data that start with \"X\"\n",
    "    protein_columns = [col for col in filtered_merged_data.columns if col.startswith(\"X\")]\n",
    "    \n",
    "    # Filter protein columns to keep only those present in the analytes list\n",
    "    filtered_protein_columns = [col for col in protein_columns if col in analytes]\n",
    "    \n",
    "    # Define required columns: metadata + filtered protein columns\n",
    "    metadata_columns = [\"participant_id\", \"session_id\", \"age\", \"diagnosis\"]\n",
    "    required_columns = metadata_columns + filtered_protein_columns\n",
    "    \n",
    "    # Filter the merged dataset to keep only required columns\n",
    "    filtered_data = filtered_merged_data[required_columns]\n",
    "    \n",
    "    # Rename \"age\" column to \"diagnosis\"\n",
    "    filtered_data.rename(columns={\"age\": \"diagnosis\"}, inplace=True)\n",
    "    \n",
    "    # Remove duplicate \"diagnosis\" column if it exists\n",
    "    filtered_data = filtered_data.loc[:, ~filtered_data.columns.duplicated()]\n",
    "    \n",
    "    # Save the filtered dataset as a TSV file\n",
    "    output_path = os.path.join(output_directory, f\"filtered_{os.path.splitext(adni_file)[0]}.tsv\")\n",
    "    filtered_data.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    \n",
    "    print(f\"Filtered dataset saved as '{output_path}'\")\n",
    "\n",
    "print(\"Processing complete for all ADNI datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run regression for all the body systems from ADNI_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running svr_linear regression for cardiovascular...\n",
      "MLNI for a regression with nested CV...\n",
      "Data split was performed based on validation strategy: hold_out...\n",
      "\n",
      "Data split has been done!\n",
      "\n",
      "Starts regression with linear SVR...\n",
      "\t\t[                                                  ] 0.40%\n",
      "\t\t[                                                  ] 0.80%\n",
      "\t\t[                                                  ] 1.20%\n",
      "\t\t[                                                  ] 1.60%\n",
      "\t\t[=                                                 ] 2.00%\n",
      "\t\t[=                                                 ] 2.40%\n",
      "\t\t[=                                                 ] 2.80%\n",
      "\t\t[=                                                 ] 3.20%\n",
      "\t\t[=                                                 ] 3.60%\n",
      "\t\t[==                                                ] 4.00%\n",
      "\t\t[==                                                ] 4.40%\n",
      "\t\t[==                                                ] 4.80%\n",
      "\t\t[==                                                ] 5.20%\n",
      "\t\t[==                                                ] 5.60%\n",
      "\t\t[===                                               ] 6.00%\n",
      "\t\t[===                                               ] 6.40%\n",
      "\t\t[===                                               ] 6.80%\n",
      "\t\t[===                                               ] 7.20%\n",
      "\t\t[===                                               ] 7.60%\n",
      "\t\t[====                                              ] 8.00%\n",
      "\t\t[====                                              ] 8.40%\n",
      "\t\t[====                                              ] 8.80%\n",
      "\t\t[====                                              ] 9.20%\n",
      "\t\t[====                                              ] 9.60%\n",
      "\t\t[=====                                             ] 10.00%\n",
      "\t\t[=====                                             ] 10.40%\n",
      "\t\t[=====                                             ] 10.80%\n",
      "\t\t[=====                                             ] 11.20%\n",
      "\t\t[=====                                             ] 11.60%\n",
      "\t\t[======                                            ] 12.00%\n",
      "\t\t[======                                            ] 12.40%\n",
      "\t\t[======                                            ] 12.80%\n",
      "\t\t[======                                            ] 13.20%\n",
      "\t\t[======                                            ] 13.60%\n",
      "\t\t[=======                                           ] 14.00%\n",
      "\t\t[=======                                           ] 14.40%\n",
      "\t\t[=======                                           ] 14.80%\n",
      "\t\t[=======                                           ] 15.20%\n",
      "\t\t[=======                                           ] 15.60%\n",
      "\t\t[========                                          ] 16.00%\n",
      "\t\t[========                                          ] 16.40%\n",
      "\t\t[========                                          ] 16.80%\n",
      "\t\t[========                                          ] 17.20%\n",
      "\t\t[========                                          ] 17.60%\n",
      "\t\t[=========                                         ] 18.00%\n",
      "\t\t[=========                                         ] 18.40%\n",
      "\t\t[=========                                         ] 18.80%\n",
      "\t\t[=========                                         ] 19.20%\n",
      "\t\t[=========                                         ] 19.60%\n",
      "\t\t[==========                                        ] 20.00%\n",
      "\t\t[==========                                        ] 20.40%\n",
      "\t\t[==========                                        ] 20.80%\n",
      "\t\t[==========                                        ] 21.20%\n",
      "\t\t[==========                                        ] 21.60%\n",
      "\t\t[===========                                       ] 22.00%\n",
      "\t\t[===========                                       ] 22.40%\n",
      "\t\t[===========                                       ] 22.80%\n",
      "\t\t[===========                                       ] 23.20%\n",
      "\t\t[===========                                       ] 23.60%\n",
      "\t\t[============                                      ] 24.00%\n",
      "\t\t[============                                      ] 24.40%\n",
      "\t\t[============                                      ] 24.80%\n",
      "\t\t[============                                      ] 25.20%\n",
      "\t\t[============                                      ] 25.60%\n",
      "\t\t[=============                                     ] 26.00%\n",
      "\t\t[=============                                     ] 26.40%\n",
      "\t\t[=============                                     ] 26.80%\n",
      "\t\t[=============                                     ] 27.20%\n",
      "\t\t[=============                                     ] 27.60%\n",
      "\t\t[==============                                    ] 28.00%\n",
      "\t\t[==============                                    ] 28.40%\n",
      "\t\t[==============                                    ] 28.80%\n",
      "\t\t[==============                                    ] 29.20%\n",
      "\t\t[==============                                    ] 29.60%\n",
      "\t\t[===============                                   ] 30.00%\n",
      "\t\t[===============                                   ] 30.40%\n",
      "\t\t[===============                                   ] 30.80%\n",
      "\t\t[===============                                   ] 31.20%\n",
      "\t\t[===============                                   ] 31.60%\n",
      "\t\t[================                                  ] 32.00%\n",
      "\t\t[================                                  ] 32.40%\n",
      "\t\t[================                                  ] 32.80%\n",
      "\t\t[================                                  ] 33.20%\n",
      "\t\t[================                                  ] 33.60%\n",
      "\t\t[=================                                 ] 34.00%\n",
      "\t\t[=================                                 ] 34.40%\n",
      "\t\t[=================                                 ] 34.80%\n",
      "\t\t[=================                                 ] 35.20%\n",
      "\t\t[=================                                 ] 35.60%\n",
      "\t\t[==================                                ] 36.00%\n",
      "\t\t[==================                                ] 36.40%\n",
      "\t\t[==================                                ] 36.80%\n",
      "\t\t[==================                                ] 37.20%\n",
      "\t\t[==================                                ] 37.60%\n",
      "\t\t[===================                               ] 38.00%\n",
      "\t\t[===================                               ] 38.40%\n",
      "\t\t[===================                               ] 38.80%\n",
      "\t\t[===================                               ] 39.20%\n",
      "\t\t[===================                               ] 39.60%\n",
      "\t\t[====================                              ] 40.00%\n",
      "\t\t[====================                              ] 40.40%\n",
      "\t\t[====================                              ] 40.80%\n",
      "\t\t[====================                              ] 41.20%\n",
      "\t\t[====================                              ] 41.60%\n",
      "\t\t[=====================                             ] 42.00%\n",
      "\t\t[=====================                             ] 42.40%\n",
      "\t\t[=====================                             ] 42.80%\n",
      "\t\t[=====================                             ] 43.20%\n",
      "\t\t[=====================                             ] 43.60%\n",
      "\t\t[======================                            ] 44.00%\n",
      "\t\t[======================                            ] 44.40%\n",
      "\t\t[======================                            ] 44.80%\n",
      "\t\t[======================                            ] 45.20%\n",
      "\t\t[======================                            ] 45.60%\n",
      "\t\t[=======================                           ] 46.00%\n",
      "\t\t[=======================                           ] 46.40%\n",
      "\t\t[=======================                           ] 46.80%\n",
      "\t\t[=======================                           ] 47.20%\n",
      "\t\t[=======================                           ] 47.60%\n",
      "\t\t[========================                          ] 48.00%\n",
      "\t\t[========================                          ] 48.40%\n",
      "\t\t[========================                          ] 48.80%\n",
      "\t\t[========================                          ] 49.20%\n",
      "\t\t[========================                          ] 49.60%\n",
      "\t\t[=========================                         ] 50.00%\n",
      "\t\t[=========================                         ] 50.40%\n",
      "\t\t[=========================                         ] 50.80%\n",
      "\t\t[=========================                         ] 51.20%\n",
      "\t\t[=========================                         ] 51.60%\n",
      "\t\t[==========================                        ] 52.00%\n",
      "\t\t[==========================                        ] 52.40%\n",
      "\t\t[==========================                        ] 52.80%\n",
      "\t\t[==========================                        ] 53.20%\n",
      "\t\t[==========================                        ] 53.60%\n",
      "\t\t[===========================                       ] 54.00%\n",
      "\t\t[===========================                       ] 54.40%\n",
      "\t\t[===========================                       ] 54.80%\n",
      "\t\t[===========================                       ] 55.20%\n",
      "\t\t[===========================                       ] 55.60%\n",
      "\t\t[============================                      ] 56.00%\n",
      "\t\t[============================                      ] 56.40%\n",
      "\t\t[============================                      ] 56.80%\n",
      "\t\t[============================                      ] 57.20%\n",
      "\t\t[============================                      ] 57.60%\n",
      "\t\t[============================                      ] 58.00%\n",
      "\t\t[=============================                     ] 58.40%\n",
      "\t\t[=============================                     ] 58.80%\n",
      "\t\t[=============================                     ] 59.20%\n",
      "\t\t[=============================                     ] 59.60%\n",
      "\t\t[==============================                    ] 60.00%\n",
      "\t\t[==============================                    ] 60.40%\n",
      "\t\t[==============================                    ] 60.80%\n",
      "\t\t[==============================                    ] 61.20%\n",
      "\t\t[==============================                    ] 61.60%\n",
      "\t\t[===============================                   ] 62.00%\n",
      "\t\t[===============================                   ] 62.40%\n",
      "\t\t[===============================                   ] 62.80%\n",
      "\t\t[===============================                   ] 63.20%\n",
      "\t\t[===============================                   ] 63.60%\n",
      "\t\t[================================                  ] 64.00%\n",
      "\t\t[================================                  ] 64.40%\n",
      "\t\t[================================                  ] 64.80%\n",
      "\t\t[================================                  ] 65.20%\n",
      "\t\t[================================                  ] 65.60%\n",
      "\t\t[=================================                 ] 66.00%\n",
      "\t\t[=================================                 ] 66.40%\n",
      "\t\t[=================================                 ] 66.80%\n",
      "\t\t[=================================                 ] 67.20%\n",
      "\t\t[=================================                 ] 67.60%\n",
      "\t\t[==================================                ] 68.00%\n",
      "\t\t[==================================                ] 68.40%\n",
      "\t\t[==================================                ] 68.80%\n",
      "\t\t[==================================                ] 69.20%\n",
      "\t\t[==================================                ] 69.60%\n",
      "\t\t[===================================               ] 70.00%\n",
      "\t\t[===================================               ] 70.40%\n",
      "\t\t[===================================               ] 70.80%\n",
      "\t\t[===================================               ] 71.20%\n",
      "\t\t[===================================               ] 71.60%\n",
      "\t\t[====================================              ] 72.00%\n",
      "\t\t[====================================              ] 72.40%\n",
      "\t\t[====================================              ] 72.80%\n",
      "\t\t[====================================              ] 73.20%\n",
      "\t\t[====================================              ] 73.60%\n",
      "\t\t[=====================================             ] 74.00%\n",
      "\t\t[=====================================             ] 74.40%\n",
      "\t\t[=====================================             ] 74.80%\n",
      "\t\t[=====================================             ] 75.20%\n",
      "\t\t[=====================================             ] 75.60%\n",
      "\t\t[======================================            ] 76.00%\n",
      "\t\t[======================================            ] 76.40%\n",
      "\t\t[======================================            ] 76.80%\n",
      "\t\t[======================================            ] 77.20%\n",
      "\t\t[======================================            ] 77.60%\n",
      "\t\t[=======================================           ] 78.00%\n",
      "\t\t[=======================================           ] 78.40%\n",
      "\t\t[=======================================           ] 78.80%\n",
      "\t\t[=======================================           ] 79.20%\n",
      "\t\t[=======================================           ] 79.60%\n",
      "\t\t[========================================          ] 80.00%\n",
      "\t\t[========================================          ] 80.40%\n",
      "\t\t[========================================          ] 80.80%\n",
      "\t\t[========================================          ] 81.20%\n",
      "\t\t[========================================          ] 81.60%\n",
      "\t\t[=========================================         ] 82.00%\n",
      "\t\t[=========================================         ] 82.40%\n",
      "\t\t[=========================================         ] 82.80%\n",
      "\t\t[=========================================         ] 83.20%\n",
      "\t\t[=========================================         ] 83.60%\n",
      "\t\t[==========================================        ] 84.00%\n",
      "\t\t[==========================================        ] 84.40%\n",
      "\t\t[==========================================        ] 84.80%\n",
      "\t\t[==========================================        ] 85.20%\n",
      "\t\t[==========================================        ] 85.60%\n",
      "\t\t[===========================================       ] 86.00%\n",
      "\t\t[===========================================       ] 86.40%\n",
      "\t\t[===========================================       ] 86.80%\n",
      "\t\t[===========================================       ] 87.20%\n",
      "\t\t[===========================================       ] 87.60%\n",
      "\t\t[============================================      ] 88.00%\n",
      "\t\t[============================================      ] 88.40%\n",
      "\t\t[============================================      ] 88.80%\n",
      "\t\t[============================================      ] 89.20%\n",
      "\t\t[============================================      ] 89.60%\n",
      "\t\t[=============================================     ] 90.00%\n",
      "\t\t[=============================================     ] 90.40%\n",
      "\t\t[=============================================     ] 90.80%\n",
      "\t\t[=============================================     ] 91.20%\n",
      "\t\t[=============================================     ] 91.60%\n",
      "\t\t[==============================================    ] 92.00%\n",
      "\t\t[==============================================    ] 92.40%\n",
      "\t\t[==============================================    ] 92.80%\n",
      "\t\t[==============================================    ] 93.20%\n",
      "\t\t[==============================================    ] 93.60%\n",
      "\t\t[===============================================   ] 94.00%\n",
      "\t\t[===============================================   ] 94.40%\n",
      "\t\t[===============================================   ] 94.80%\n",
      "\t\t[===============================================   ] 95.20%\n",
      "\t\t[===============================================   ] 95.60%\n",
      "\t\t[================================================  ] 96.00%\n",
      "\t\t[================================================  ] 96.40%\n",
      "\t\t[================================================  ] 96.80%\n",
      "\t\t[================================================  ] 97.20%\n",
      "\t\t[================================================  ] 97.60%\n",
      "\t\t[================================================= ] 98.00%\n",
      "\t\t[================================================= ] 98.40%\n",
      "\t\t[================================================= ] 98.80%\n",
      "\t\t[================================================= ] 99.20%\n",
      "\t\t[================================================= ] 99.60%\n",
      "\t\t[==================================================] 100.00%\n",
      "Mean absolute error for the average model: 5.447733\n",
      "Mean absolute error for the single model: 5.470988\n",
      "Finish...\n",
      "svr_linear regression completed for cardiovascular. Results saved in result_by_type/cardiovascular_svr_linear.\n",
      "Running svr_rbf regression for cardiovascular...\n",
      "MLNI for a regression with nested CV...\n",
      "Data split was performed based on validation strategy: hold_out...\n",
      "\n",
      "Data split has been done!\n",
      "\n",
      "Starts regression with SVR...\n",
      "\t\t[                                                  ] 0.40%\n",
      "\t\t[                                                  ] 0.80%\n",
      "\t\t[                                                  ] 1.20%\n",
      "\t\t[                                                  ] 1.60%\n",
      "\t\t[=                                                 ] 2.00%\n",
      "\t\t[=                                                 ] 2.40%\n",
      "\t\t[=                                                 ] 2.80%\n",
      "\t\t[=                                                 ] 3.20%\n",
      "\t\t[=                                                 ] 3.60%\n",
      "\t\t[==                                                ] 4.00%\n",
      "\t\t[==                                                ] 4.40%\n",
      "\t\t[==                                                ] 4.80%\n",
      "\t\t[==                                                ] 5.20%\n",
      "\t\t[==                                                ] 5.60%\n",
      "\t\t[===                                               ] 6.00%\n",
      "\t\t[===                                               ] 6.40%\n",
      "\t\t[===                                               ] 6.80%\n",
      "\t\t[===                                               ] 7.20%\n",
      "\t\t[===                                               ] 7.60%\n",
      "\t\t[====                                              ] 8.00%\n",
      "\t\t[====                                              ] 8.40%\n",
      "\t\t[====                                              ] 8.80%\n",
      "\t\t[====                                              ] 9.20%\n",
      "\t\t[====                                              ] 9.60%\n",
      "\t\t[=====                                             ] 10.00%\n",
      "\t\t[=====                                             ] 10.40%\n",
      "\t\t[=====                                             ] 10.80%\n",
      "\t\t[=====                                             ] 11.20%\n",
      "\t\t[=====                                             ] 11.60%\n",
      "\t\t[======                                            ] 12.00%\n",
      "\t\t[======                                            ] 12.40%\n",
      "\t\t[======                                            ] 12.80%\n",
      "\t\t[======                                            ] 13.20%\n",
      "\t\t[======                                            ] 13.60%\n",
      "\t\t[=======                                           ] 14.00%\n",
      "\t\t[=======                                           ] 14.40%\n",
      "\t\t[=======                                           ] 14.80%\n",
      "\t\t[=======                                           ] 15.20%\n",
      "\t\t[=======                                           ] 15.60%\n",
      "\t\t[========                                          ] 16.00%\n",
      "\t\t[========                                          ] 16.40%\n",
      "\t\t[========                                          ] 16.80%\n",
      "\t\t[========                                          ] 17.20%\n",
      "\t\t[========                                          ] 17.60%\n",
      "\t\t[=========                                         ] 18.00%\n",
      "\t\t[=========                                         ] 18.40%\n",
      "\t\t[=========                                         ] 18.80%\n",
      "\t\t[=========                                         ] 19.20%\n",
      "\t\t[=========                                         ] 19.60%\n",
      "\t\t[==========                                        ] 20.00%\n",
      "\t\t[==========                                        ] 20.40%\n",
      "\t\t[==========                                        ] 20.80%\n",
      "\t\t[==========                                        ] 21.20%\n",
      "\t\t[==========                                        ] 21.60%\n",
      "\t\t[===========                                       ] 22.00%\n",
      "\t\t[===========                                       ] 22.40%\n",
      "\t\t[===========                                       ] 22.80%\n",
      "\t\t[===========                                       ] 23.20%\n",
      "\t\t[===========                                       ] 23.60%\n",
      "\t\t[============                                      ] 24.00%\n",
      "\t\t[============                                      ] 24.40%\n",
      "\t\t[============                                      ] 24.80%\n",
      "\t\t[============                                      ] 25.20%\n",
      "\t\t[============                                      ] 25.60%\n",
      "\t\t[=============                                     ] 26.00%\n",
      "\t\t[=============                                     ] 26.40%\n",
      "\t\t[=============                                     ] 26.80%\n",
      "\t\t[=============                                     ] 27.20%\n",
      "\t\t[=============                                     ] 27.60%\n",
      "\t\t[==============                                    ] 28.00%\n",
      "\t\t[==============                                    ] 28.40%\n",
      "\t\t[==============                                    ] 28.80%\n",
      "\t\t[==============                                    ] 29.20%\n",
      "\t\t[==============                                    ] 29.60%\n",
      "\t\t[===============                                   ] 30.00%\n",
      "\t\t[===============                                   ] 30.40%\n",
      "\t\t[===============                                   ] 30.80%\n",
      "\t\t[===============                                   ] 31.20%\n",
      "\t\t[===============                                   ] 31.60%\n",
      "\t\t[================                                  ] 32.00%\n",
      "\t\t[================                                  ] 32.40%\n",
      "\t\t[================                                  ] 32.80%\n",
      "\t\t[================                                  ] 33.20%\n",
      "\t\t[================                                  ] 33.60%\n",
      "\t\t[=================                                 ] 34.00%\n",
      "\t\t[=================                                 ] 34.40%\n",
      "\t\t[=================                                 ] 34.80%\n",
      "\t\t[=================                                 ] 35.20%\n",
      "\t\t[=================                                 ] 35.60%\n",
      "\t\t[==================                                ] 36.00%\n",
      "\t\t[==================                                ] 36.40%\n",
      "\t\t[==================                                ] 36.80%\n",
      "\t\t[==================                                ] 37.20%\n",
      "\t\t[==================                                ] 37.60%\n",
      "\t\t[===================                               ] 38.00%\n",
      "\t\t[===================                               ] 38.40%\n",
      "\t\t[===================                               ] 38.80%\n",
      "\t\t[===================                               ] 39.20%\n",
      "\t\t[===================                               ] 39.60%\n",
      "\t\t[====================                              ] 40.00%\n",
      "\t\t[====================                              ] 40.40%\n",
      "\t\t[====================                              ] 40.80%\n",
      "\t\t[====================                              ] 41.20%\n",
      "\t\t[====================                              ] 41.60%\n",
      "\t\t[=====================                             ] 42.00%\n",
      "\t\t[=====================                             ] 42.40%\n",
      "\t\t[=====================                             ] 42.80%\n",
      "\t\t[=====================                             ] 43.20%\n",
      "\t\t[=====================                             ] 43.60%\n",
      "\t\t[======================                            ] 44.00%\n",
      "\t\t[======================                            ] 44.40%\n",
      "\t\t[======================                            ] 44.80%\n",
      "\t\t[======================                            ] 45.20%\n",
      "\t\t[======================                            ] 45.60%\n",
      "\t\t[=======================                           ] 46.00%\n",
      "\t\t[=======================                           ] 46.40%\n",
      "\t\t[=======================                           ] 46.80%\n",
      "\t\t[=======================                           ] 47.20%\n",
      "\t\t[=======================                           ] 47.60%\n",
      "\t\t[========================                          ] 48.00%\n",
      "\t\t[========================                          ] 48.40%\n",
      "\t\t[========================                          ] 48.80%\n",
      "\t\t[========================                          ] 49.20%\n",
      "\t\t[========================                          ] 49.60%\n",
      "\t\t[=========================                         ] 50.00%\n",
      "\t\t[=========================                         ] 50.40%\n",
      "\t\t[=========================                         ] 50.80%\n",
      "\t\t[=========================                         ] 51.20%\n",
      "\t\t[=========================                         ] 51.60%\n",
      "\t\t[==========================                        ] 52.00%\n",
      "\t\t[==========================                        ] 52.40%\n",
      "\t\t[==========================                        ] 52.80%\n",
      "\t\t[==========================                        ] 53.20%\n",
      "\t\t[==========================                        ] 53.60%\n",
      "\t\t[===========================                       ] 54.00%\n",
      "\t\t[===========================                       ] 54.40%\n",
      "\t\t[===========================                       ] 54.80%\n",
      "\t\t[===========================                       ] 55.20%\n",
      "\t\t[===========================                       ] 55.60%\n",
      "\t\t[============================                      ] 56.00%\n",
      "\t\t[============================                      ] 56.40%\n",
      "\t\t[============================                      ] 56.80%\n",
      "\t\t[============================                      ] 57.20%\n",
      "\t\t[============================                      ] 57.60%\n",
      "\t\t[============================                      ] 58.00%\n",
      "\t\t[=============================                     ] 58.40%\n",
      "\t\t[=============================                     ] 58.80%\n",
      "\t\t[=============================                     ] 59.20%\n",
      "\t\t[=============================                     ] 59.60%\n",
      "\t\t[==============================                    ] 60.00%\n",
      "\t\t[==============================                    ] 60.40%\n",
      "\t\t[==============================                    ] 60.80%\n",
      "\t\t[==============================                    ] 61.20%\n",
      "\t\t[==============================                    ] 61.60%\n",
      "\t\t[===============================                   ] 62.00%\n",
      "\t\t[===============================                   ] 62.40%\n",
      "\t\t[===============================                   ] 62.80%\n",
      "\t\t[===============================                   ] 63.20%\n",
      "\t\t[===============================                   ] 63.60%\n",
      "\t\t[================================                  ] 64.00%\n",
      "\t\t[================================                  ] 64.40%\n",
      "\t\t[================================                  ] 64.80%\n",
      "\t\t[================================                  ] 65.20%\n",
      "\t\t[================================                  ] 65.60%\n",
      "\t\t[=================================                 ] 66.00%\n",
      "\t\t[=================================                 ] 66.40%\n",
      "\t\t[=================================                 ] 66.80%\n",
      "\t\t[=================================                 ] 67.20%\n",
      "\t\t[=================================                 ] 67.60%\n",
      "\t\t[==================================                ] 68.00%\n",
      "\t\t[==================================                ] 68.40%\n",
      "\t\t[==================================                ] 68.80%\n",
      "\t\t[==================================                ] 69.20%\n",
      "\t\t[==================================                ] 69.60%\n",
      "\t\t[===================================               ] 70.00%\n",
      "\t\t[===================================               ] 70.40%\n",
      "\t\t[===================================               ] 70.80%\n",
      "\t\t[===================================               ] 71.20%\n",
      "\t\t[===================================               ] 71.60%\n",
      "\t\t[====================================              ] 72.00%\n",
      "\t\t[====================================              ] 72.40%\n",
      "\t\t[====================================              ] 72.80%\n",
      "\t\t[====================================              ] 73.20%\n",
      "\t\t[====================================              ] 73.60%\n",
      "\t\t[=====================================             ] 74.00%\n",
      "\t\t[=====================================             ] 74.40%\n",
      "\t\t[=====================================             ] 74.80%\n",
      "\t\t[=====================================             ] 75.20%\n",
      "\t\t[=====================================             ] 75.60%\n",
      "\t\t[======================================            ] 76.00%\n",
      "\t\t[======================================            ] 76.40%\n",
      "\t\t[======================================            ] 76.80%\n",
      "\t\t[======================================            ] 77.20%\n",
      "\t\t[======================================            ] 77.60%\n",
      "\t\t[=======================================           ] 78.00%\n",
      "\t\t[=======================================           ] 78.40%\n",
      "\t\t[=======================================           ] 78.80%\n",
      "\t\t[=======================================           ] 79.20%\n",
      "\t\t[=======================================           ] 79.60%\n",
      "\t\t[========================================          ] 80.00%\n",
      "\t\t[========================================          ] 80.40%\n",
      "\t\t[========================================          ] 80.80%\n",
      "\t\t[========================================          ] 81.20%\n",
      "\t\t[========================================          ] 81.60%\n",
      "\t\t[=========================================         ] 82.00%\n",
      "\t\t[=========================================         ] 82.40%\n",
      "\t\t[=========================================         ] 82.80%\n",
      "\t\t[=========================================         ] 83.20%\n",
      "\t\t[=========================================         ] 83.60%\n",
      "\t\t[==========================================        ] 84.00%\n",
      "\t\t[==========================================        ] 84.40%\n",
      "\t\t[==========================================        ] 84.80%\n",
      "\t\t[==========================================        ] 85.20%\n",
      "\t\t[==========================================        ] 85.60%\n",
      "\t\t[===========================================       ] 86.00%\n",
      "\t\t[===========================================       ] 86.40%\n",
      "\t\t[===========================================       ] 86.80%\n",
      "\t\t[===========================================       ] 87.20%\n",
      "\t\t[===========================================       ] 87.60%\n",
      "\t\t[============================================      ] 88.00%\n",
      "\t\t[============================================      ] 88.40%\n",
      "\t\t[============================================      ] 88.80%\n",
      "\t\t[============================================      ] 89.20%\n",
      "\t\t[============================================      ] 89.60%\n",
      "\t\t[=============================================     ] 90.00%\n",
      "\t\t[=============================================     ] 90.40%\n",
      "\t\t[=============================================     ] 90.80%\n",
      "\t\t[=============================================     ] 91.20%\n",
      "\t\t[=============================================     ] 91.60%\n",
      "\t\t[==============================================    ] 92.00%\n",
      "\t\t[==============================================    ] 92.40%\n",
      "\t\t[==============================================    ] 92.80%\n",
      "\t\t[==============================================    ] 93.20%\n",
      "\t\t[==============================================    ] 93.60%\n",
      "\t\t[===============================================   ] 94.00%\n",
      "\t\t[===============================================   ] 94.40%\n",
      "\t\t[===============================================   ] 94.80%\n",
      "\t\t[===============================================   ] 95.20%\n",
      "\t\t[===============================================   ] 95.60%\n",
      "\t\t[================================================  ] 96.00%\n",
      "\t\t[================================================  ] 96.40%\n",
      "\t\t[================================================  ] 96.80%\n",
      "\t\t[================================================  ] 97.20%\n",
      "\t\t[================================================  ] 97.60%\n",
      "\t\t[================================================= ] 98.00%\n",
      "\t\t[================================================= ] 98.40%\n",
      "\t\t[================================================= ] 98.80%\n",
      "\t\t[================================================= ] 99.20%\n",
      "\t\t[================================================= ] 99.60%\n",
      "\t\t[==================================================] 100.00%\n",
      "Error running svr_rbf regression for cardiovascular: The 'max_iter' parameter of SVR must be an int in the range [-1, inf). Got 1000000.0 instead.\n",
      "Running nn regression for cardiovascular...\n",
      "MLNI for a regression with nested CV...\n",
      "Data split was performed based on validation strategy: hold_out...\n",
      "\n",
      "Data split has been done!\n",
      "\n",
      "Starts regression with NN...\n",
      "\t\t[                                                  ] 0.40%\n",
      "Running for the 0-th repetition of the repeated holdout CV\n",
      "At 0-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 73.261113 during each epoch using batch data 0\n",
      "For training, mean MAE loss: 73.616443 at the end of epoch by applying all training data 0\n",
      "For validation, mean MAE loss: 73.029905 at the end of epoch 0\n",
      "At 1-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 73.055531 during each epoch using batch data 1\n",
      "For training, mean MAE loss: 73.202435 at the end of epoch by applying all training data 1\n",
      "For validation, mean MAE loss: 72.978856 at the end of epoch 1\n",
      "At 2-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 73.111389 during each epoch using batch data 2\n",
      "For training, mean MAE loss: 73.082053 at the end of epoch by applying all training data 2\n",
      "For validation, mean MAE loss: 72.924820 at the end of epoch 2\n",
      "At 3-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 72.873243 during each epoch using batch data 3\n",
      "For training, mean MAE loss: 72.977953 at the end of epoch by applying all training data 3\n",
      "For validation, mean MAE loss: 72.864349 at the end of epoch 3\n",
      "At 4-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 73.290781 during each epoch using batch data 4\n",
      "For training, mean MAE loss: 72.675356 at the end of epoch by applying all training data 4\n",
      "For validation, mean MAE loss: 72.793376 at the end of epoch 4\n",
      "At 5-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 72.877092 during each epoch using batch data 5\n",
      "For training, mean MAE loss: 72.662938 at the end of epoch by applying all training data 5\n",
      "For validation, mean MAE loss: 72.709310 at the end of epoch 5\n",
      "At 6-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 72.966142 during each epoch using batch data 6\n",
      "For training, mean MAE loss: 72.817017 at the end of epoch by applying all training data 6\n",
      "For validation, mean MAE loss: 72.609886 at the end of epoch 6\n",
      "At 7-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 72.797716 during each epoch using batch data 7\n",
      "For training, mean MAE loss: 72.782834 at the end of epoch by applying all training data 7\n",
      "For validation, mean MAE loss: 72.496290 at the end of epoch 7\n",
      "At 8-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 72.568840 during each epoch using batch data 8\n",
      "For training, mean MAE loss: 72.406502 at the end of epoch by applying all training data 8\n",
      "For validation, mean MAE loss: 72.369240 at the end of epoch 8\n",
      "At 9-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 72.420235 during each epoch using batch data 9\n",
      "For training, mean MAE loss: 72.362955 at the end of epoch by applying all training data 9\n",
      "For validation, mean MAE loss: 72.227023 at the end of epoch 9\n",
      "At 10-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 72.532783 during each epoch using batch data 10\n",
      "For training, mean MAE loss: 72.228243 at the end of epoch by applying all training data 10\n",
      "For validation, mean MAE loss: 72.068232 at the end of epoch 10\n",
      "At 11-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 72.216750 during each epoch using batch data 11\n",
      "For training, mean MAE loss: 71.790695 at the end of epoch by applying all training data 11\n",
      "For validation, mean MAE loss: 71.889870 at the end of epoch 11\n",
      "At 12-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 72.095264 during each epoch using batch data 12\n",
      "For training, mean MAE loss: 71.940012 at the end of epoch by applying all training data 12\n",
      "For validation, mean MAE loss: 71.690025 at the end of epoch 12\n",
      "At 13-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 71.753506 during each epoch using batch data 13\n",
      "For training, mean MAE loss: 71.637552 at the end of epoch by applying all training data 13\n",
      "For validation, mean MAE loss: 71.465970 at the end of epoch 13\n",
      "At 14-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 71.543465 during each epoch using batch data 14\n",
      "For training, mean MAE loss: 71.329334 at the end of epoch by applying all training data 14\n",
      "For validation, mean MAE loss: 71.214920 at the end of epoch 14\n",
      "At 15-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 71.387096 during each epoch using batch data 15\n",
      "For training, mean MAE loss: 70.918635 at the end of epoch by applying all training data 15\n",
      "For validation, mean MAE loss: 70.934924 at the end of epoch 15\n",
      "At 16-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 70.838528 during each epoch using batch data 16\n",
      "For training, mean MAE loss: 70.667719 at the end of epoch by applying all training data 16\n",
      "For validation, mean MAE loss: 70.623248 at the end of epoch 16\n",
      "At 17-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 70.669577 during each epoch using batch data 17\n",
      "For training, mean MAE loss: 70.705843 at the end of epoch by applying all training data 17\n",
      "For validation, mean MAE loss: 70.277929 at the end of epoch 17\n",
      "At 18-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 69.894705 during each epoch using batch data 18\n",
      "For training, mean MAE loss: 69.919286 at the end of epoch by applying all training data 18\n",
      "For validation, mean MAE loss: 69.891309 at the end of epoch 18\n",
      "At 19-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 69.343809 during each epoch using batch data 19\n",
      "For training, mean MAE loss: 69.793333 at the end of epoch by applying all training data 19\n",
      "For validation, mean MAE loss: 69.457087 at the end of epoch 19\n",
      "At 20-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 69.332784 during each epoch using batch data 20\n",
      "For training, mean MAE loss: 69.108727 at the end of epoch by applying all training data 20\n",
      "For validation, mean MAE loss: 68.981911 at the end of epoch 20\n",
      "At 21-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 68.732946 during each epoch using batch data 21\n",
      "For training, mean MAE loss: 68.425842 at the end of epoch by applying all training data 21\n",
      "For validation, mean MAE loss: 68.465289 at the end of epoch 21\n",
      "At 22-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 68.289111 during each epoch using batch data 22\n",
      "For training, mean MAE loss: 68.034846 at the end of epoch by applying all training data 22\n",
      "For validation, mean MAE loss: 67.906044 at the end of epoch 22\n",
      "At 23-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 67.981197 during each epoch using batch data 23\n",
      "For training, mean MAE loss: 67.253363 at the end of epoch by applying all training data 23\n",
      "For validation, mean MAE loss: 67.298566 at the end of epoch 23\n",
      "At 24-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 66.908285 during each epoch using batch data 24\n",
      "For training, mean MAE loss: 66.839445 at the end of epoch by applying all training data 24\n",
      "For validation, mean MAE loss: 66.644409 at the end of epoch 24\n",
      "At 25-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 66.404534 during each epoch using batch data 25\n",
      "For training, mean MAE loss: 65.865868 at the end of epoch by applying all training data 25\n",
      "For validation, mean MAE loss: 65.941589 at the end of epoch 25\n",
      "At 26-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 65.580254 during each epoch using batch data 26\n",
      "For training, mean MAE loss: 65.258820 at the end of epoch by applying all training data 26\n",
      "For validation, mean MAE loss: 65.187518 at the end of epoch 26\n",
      "At 27-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 64.931267 during each epoch using batch data 27\n",
      "For training, mean MAE loss: 64.629261 at the end of epoch by applying all training data 27\n",
      "For validation, mean MAE loss: 64.380156 at the end of epoch 27\n",
      "At 28-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 64.106474 during each epoch using batch data 28\n",
      "For training, mean MAE loss: 63.448697 at the end of epoch by applying all training data 28\n",
      "For validation, mean MAE loss: 63.518032 at the end of epoch 28\n",
      "At 29-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 63.261012 during each epoch using batch data 29\n",
      "For training, mean MAE loss: 62.859620 at the end of epoch by applying all training data 29\n",
      "For validation, mean MAE loss: 62.597446 at the end of epoch 29\n",
      "At 30-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 62.353811 during each epoch using batch data 30\n",
      "For training, mean MAE loss: 61.807970 at the end of epoch by applying all training data 30\n",
      "For validation, mean MAE loss: 61.616543 at the end of epoch 30\n",
      "At 31-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 61.537041 during each epoch using batch data 31\n",
      "For training, mean MAE loss: 60.553734 at the end of epoch by applying all training data 31\n",
      "For validation, mean MAE loss: 60.574389 at the end of epoch 31\n",
      "At 32-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 59.858149 during each epoch using batch data 32\n",
      "For training, mean MAE loss: 59.543520 at the end of epoch by applying all training data 32\n",
      "For validation, mean MAE loss: 59.467158 at the end of epoch 32\n",
      "At 33-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 58.989226 during each epoch using batch data 33\n",
      "For training, mean MAE loss: 58.399625 at the end of epoch by applying all training data 33\n",
      "For validation, mean MAE loss: 58.297351 at the end of epoch 33\n",
      "At 34-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 57.929849 during each epoch using batch data 34\n",
      "For training, mean MAE loss: 57.348719 at the end of epoch by applying all training data 34\n",
      "For validation, mean MAE loss: 57.056547 at the end of epoch 34\n",
      "At 35-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 56.483969 during each epoch using batch data 35\n",
      "For training, mean MAE loss: 55.629522 at the end of epoch by applying all training data 35\n",
      "For validation, mean MAE loss: 55.747836 at the end of epoch 35\n",
      "At 36-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 55.034449 during each epoch using batch data 36\n",
      "For training, mean MAE loss: 54.186086 at the end of epoch by applying all training data 36\n",
      "For validation, mean MAE loss: 54.364522 at the end of epoch 36\n",
      "At 37-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 53.905266 during each epoch using batch data 37\n",
      "For training, mean MAE loss: 52.938638 at the end of epoch by applying all training data 37\n",
      "For validation, mean MAE loss: 52.906957 at the end of epoch 37\n",
      "At 38-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 52.246326 during each epoch using batch data 38\n",
      "For training, mean MAE loss: 51.489305 at the end of epoch by applying all training data 38\n",
      "For validation, mean MAE loss: 51.377547 at the end of epoch 38\n",
      "At 39-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 50.675303 during each epoch using batch data 39\n",
      "For training, mean MAE loss: 49.694137 at the end of epoch by applying all training data 39\n",
      "For validation, mean MAE loss: 49.770648 at the end of epoch 39\n",
      "At 40-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 49.025544 during each epoch using batch data 40\n",
      "For training, mean MAE loss: 47.987357 at the end of epoch by applying all training data 40\n",
      "For validation, mean MAE loss: 48.088295 at the end of epoch 40\n",
      "At 41-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 47.348991 during each epoch using batch data 41\n",
      "For training, mean MAE loss: 46.286786 at the end of epoch by applying all training data 41\n",
      "For validation, mean MAE loss: 46.322468 at the end of epoch 41\n",
      "At 42-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 45.422956 during each epoch using batch data 42\n",
      "For training, mean MAE loss: 44.337649 at the end of epoch by applying all training data 42\n",
      "For validation, mean MAE loss: 44.478887 at the end of epoch 42\n",
      "At 43-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 43.611875 during each epoch using batch data 43\n",
      "For training, mean MAE loss: 42.644581 at the end of epoch by applying all training data 43\n",
      "For validation, mean MAE loss: 42.556243 at the end of epoch 43\n",
      "At 44-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 41.785820 during each epoch using batch data 44\n",
      "For training, mean MAE loss: 40.482080 at the end of epoch by applying all training data 44\n",
      "For validation, mean MAE loss: 40.543694 at the end of epoch 44\n",
      "At 45-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 39.521619 during each epoch using batch data 45\n",
      "For training, mean MAE loss: 38.266480 at the end of epoch by applying all training data 45\n",
      "For validation, mean MAE loss: 38.441415 at the end of epoch 45\n",
      "At 46-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 37.521548 during each epoch using batch data 46\n",
      "For training, mean MAE loss: 36.112932 at the end of epoch by applying all training data 46\n",
      "For validation, mean MAE loss: 36.255698 at the end of epoch 46\n",
      "At 47-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 34.920893 during each epoch using batch data 47\n",
      "For training, mean MAE loss: 33.844246 at the end of epoch by applying all training data 47\n",
      "For validation, mean MAE loss: 33.980967 at the end of epoch 47\n",
      "At 48-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 32.759546 during each epoch using batch data 48\n",
      "For training, mean MAE loss: 31.526561 at the end of epoch by applying all training data 48\n",
      "For validation, mean MAE loss: 31.618953 at the end of epoch 48\n",
      "At 49-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 30.695268 during each epoch using batch data 49\n",
      "For training, mean MAE loss: 29.143651 at the end of epoch by applying all training data 49\n",
      "For validation, mean MAE loss: 29.162781 at the end of epoch 49\n",
      "At 50-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 27.738613 during each epoch using batch data 50\n",
      "For training, mean MAE loss: 26.093426 at the end of epoch by applying all training data 50\n",
      "For validation, mean MAE loss: 26.613892 at the end of epoch 50\n",
      "At 51-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 25.499677 during each epoch using batch data 51\n",
      "For training, mean MAE loss: 23.997113 at the end of epoch by applying all training data 51\n",
      "For validation, mean MAE loss: 23.995879 at the end of epoch 51\n",
      "At 52-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 22.549107 during each epoch using batch data 52\n",
      "For training, mean MAE loss: 20.851879 at the end of epoch by applying all training data 52\n",
      "For validation, mean MAE loss: 21.279293 at the end of epoch 52\n",
      "At 53-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 19.841397 during each epoch using batch data 53\n",
      "For training, mean MAE loss: 18.185542 at the end of epoch by applying all training data 53\n",
      "For validation, mean MAE loss: 18.528556 at the end of epoch 53\n",
      "At 54-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 17.283522 during each epoch using batch data 54\n",
      "For training, mean MAE loss: 15.435345 at the end of epoch by applying all training data 54\n",
      "For validation, mean MAE loss: 15.822282 at the end of epoch 54\n",
      "At 55-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 14.220535 during each epoch using batch data 55\n",
      "For training, mean MAE loss: 12.900309 at the end of epoch by applying all training data 55\n",
      "For validation, mean MAE loss: 13.213772 at the end of epoch 55\n",
      "At 56-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 11.903647 during each epoch using batch data 56\n",
      "For training, mean MAE loss: 10.707217 at the end of epoch by applying all training data 56\n",
      "For validation, mean MAE loss: 10.932343 at the end of epoch 56\n",
      "At 57-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 9.760583 during each epoch using batch data 57\n",
      "For training, mean MAE loss: 8.965680 at the end of epoch by applying all training data 57\n",
      "For validation, mean MAE loss: 9.160748 at the end of epoch 57\n",
      "At 58-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 8.759804 during each epoch using batch data 58\n",
      "For training, mean MAE loss: 7.602412 at the end of epoch by applying all training data 58\n",
      "For validation, mean MAE loss: 8.022014 at the end of epoch 58\n",
      "At 59-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.523016 during each epoch using batch data 59\n",
      "For training, mean MAE loss: 7.157548 at the end of epoch by applying all training data 59\n",
      "For validation, mean MAE loss: 7.634260 at the end of epoch 59\n",
      "At 60-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.008421 during each epoch using batch data 60\n",
      "For training, mean MAE loss: 7.160142 at the end of epoch by applying all training data 60\n",
      "For validation, mean MAE loss: 7.505943 at the end of epoch 60\n",
      "At 61-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.975557 during each epoch using batch data 61\n",
      "For training, mean MAE loss: 7.191806 at the end of epoch by applying all training data 61\n",
      "For validation, mean MAE loss: 7.472379 at the end of epoch 61\n",
      "At 62-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.291207 during each epoch using batch data 62\n",
      "For training, mean MAE loss: 6.926541 at the end of epoch by applying all training data 62\n",
      "For validation, mean MAE loss: 7.480102 at the end of epoch 62\n",
      "At 63-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.627819 during each epoch using batch data 63\n",
      "For training, mean MAE loss: 6.823490 at the end of epoch by applying all training data 63\n",
      "For validation, mean MAE loss: 7.488527 at the end of epoch 63\n",
      "At 64-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.702759 during each epoch using batch data 64\n",
      "For training, mean MAE loss: 6.924202 at the end of epoch by applying all training data 64\n",
      "For validation, mean MAE loss: 7.490917 at the end of epoch 64\n",
      "At 65-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.894664 during each epoch using batch data 65\n",
      "For training, mean MAE loss: 6.928297 at the end of epoch by applying all training data 65\n",
      "For validation, mean MAE loss: 7.499745 at the end of epoch 65\n",
      "At 66-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.834936 during each epoch using batch data 66\n",
      "For training, mean MAE loss: 6.780473 at the end of epoch by applying all training data 66\n",
      "For validation, mean MAE loss: 7.498600 at the end of epoch 66\n",
      "At 67-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.028816 during each epoch using batch data 67\n",
      "For training, mean MAE loss: 6.832711 at the end of epoch by applying all training data 67\n",
      "For validation, mean MAE loss: 7.489494 at the end of epoch 67\n",
      "At 68-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.709999 during each epoch using batch data 68\n",
      "For training, mean MAE loss: 6.725069 at the end of epoch by applying all training data 68\n",
      "For validation, mean MAE loss: 7.480967 at the end of epoch 68\n",
      "At 69-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.691902 during each epoch using batch data 69\n",
      "For training, mean MAE loss: 6.874219 at the end of epoch by applying all training data 69\n",
      "For validation, mean MAE loss: 7.489055 at the end of epoch 69\n",
      "At 70-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.006197 during each epoch using batch data 70\n",
      "For training, mean MAE loss: 6.740060 at the end of epoch by applying all training data 70\n",
      "For validation, mean MAE loss: 7.496077 at the end of epoch 70\n",
      "At 71-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.862804 during each epoch using batch data 71\n",
      "For training, mean MAE loss: 6.997188 at the end of epoch by applying all training data 71\n",
      "For validation, mean MAE loss: 7.497743 at the end of epoch 71\n",
      "At 72-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.922338 during each epoch using batch data 72\n",
      "For training, mean MAE loss: 6.958712 at the end of epoch by applying all training data 72\n",
      "For validation, mean MAE loss: 7.495781 at the end of epoch 72\n",
      "At 73-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.951891 during each epoch using batch data 73\n",
      "For training, mean MAE loss: 6.773965 at the end of epoch by applying all training data 73\n",
      "For validation, mean MAE loss: 7.491951 at the end of epoch 73\n",
      "At 74-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.013033 during each epoch using batch data 74\n",
      "For training, mean MAE loss: 7.350080 at the end of epoch by applying all training data 74\n",
      "For validation, mean MAE loss: 7.490750 at the end of epoch 74\n",
      "At 75-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.971874 during each epoch using batch data 75\n",
      "For training, mean MAE loss: 6.758040 at the end of epoch by applying all training data 75\n",
      "For validation, mean MAE loss: 7.478654 at the end of epoch 75\n",
      "At 76-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.973861 during each epoch using batch data 76\n",
      "For training, mean MAE loss: 7.099559 at the end of epoch by applying all training data 76\n",
      "For validation, mean MAE loss: 7.470861 at the end of epoch 76\n",
      "At 77-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.877994 during each epoch using batch data 77\n",
      "For training, mean MAE loss: 6.804337 at the end of epoch by applying all training data 77\n",
      "For validation, mean MAE loss: 7.463319 at the end of epoch 77\n",
      "At 78-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.038418 during each epoch using batch data 78\n",
      "For training, mean MAE loss: 6.921082 at the end of epoch by applying all training data 78\n",
      "For validation, mean MAE loss: 7.463116 at the end of epoch 78\n",
      "At 79-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.674620 during each epoch using batch data 79\n",
      "For training, mean MAE loss: 6.918914 at the end of epoch by applying all training data 79\n",
      "For validation, mean MAE loss: 7.458881 at the end of epoch 79\n",
      "At 80-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.049566 during each epoch using batch data 80\n",
      "For training, mean MAE loss: 6.789657 at the end of epoch by applying all training data 80\n",
      "For validation, mean MAE loss: 7.462367 at the end of epoch 80\n",
      "At 81-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.755679 during each epoch using batch data 81\n",
      "For training, mean MAE loss: 6.928031 at the end of epoch by applying all training data 81\n",
      "For validation, mean MAE loss: 7.473853 at the end of epoch 81\n",
      "At 82-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.127187 during each epoch using batch data 82\n",
      "For training, mean MAE loss: 6.581901 at the end of epoch by applying all training data 82\n",
      "For validation, mean MAE loss: 7.475207 at the end of epoch 82\n",
      "At 83-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.102435 during each epoch using batch data 83\n",
      "For training, mean MAE loss: 6.679678 at the end of epoch by applying all training data 83\n",
      "For validation, mean MAE loss: 7.478639 at the end of epoch 83\n",
      "At 84-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.755254 during each epoch using batch data 84\n",
      "For training, mean MAE loss: 6.837295 at the end of epoch by applying all training data 84\n",
      "For validation, mean MAE loss: 7.481301 at the end of epoch 84\n",
      "At 85-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.823187 during each epoch using batch data 85\n",
      "For training, mean MAE loss: 6.840817 at the end of epoch by applying all training data 85\n",
      "For validation, mean MAE loss: 7.474625 at the end of epoch 85\n",
      "At 86-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.771113 during each epoch using batch data 86\n",
      "For training, mean MAE loss: 6.632187 at the end of epoch by applying all training data 86\n",
      "For validation, mean MAE loss: 7.475055 at the end of epoch 86\n",
      "At 87-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.694149 during each epoch using batch data 87\n",
      "For training, mean MAE loss: 6.892870 at the end of epoch by applying all training data 87\n",
      "For validation, mean MAE loss: 7.461624 at the end of epoch 87\n",
      "At 88-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.083333 during each epoch using batch data 88\n",
      "For training, mean MAE loss: 6.824171 at the end of epoch by applying all training data 88\n",
      "For validation, mean MAE loss: 7.455206 at the end of epoch 88\n",
      "At 89-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.827604 during each epoch using batch data 89\n",
      "For training, mean MAE loss: 7.061983 at the end of epoch by applying all training data 89\n",
      "For validation, mean MAE loss: 7.448711 at the end of epoch 89\n",
      "At 90-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.874713 during each epoch using batch data 90\n",
      "For training, mean MAE loss: 6.609308 at the end of epoch by applying all training data 90\n",
      "For validation, mean MAE loss: 7.444942 at the end of epoch 90\n",
      "At 91-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.930635 during each epoch using batch data 91\n",
      "For training, mean MAE loss: 6.964585 at the end of epoch by applying all training data 91\n",
      "For validation, mean MAE loss: 7.439617 at the end of epoch 91\n",
      "At 92-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.819100 during each epoch using batch data 92\n",
      "For training, mean MAE loss: 6.847355 at the end of epoch by applying all training data 92\n",
      "For validation, mean MAE loss: 7.438317 at the end of epoch 92\n",
      "At 93-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.773384 during each epoch using batch data 93\n",
      "For training, mean MAE loss: 6.738845 at the end of epoch by applying all training data 93\n",
      "For validation, mean MAE loss: 7.431608 at the end of epoch 93\n",
      "At 94-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.796045 during each epoch using batch data 94\n",
      "For training, mean MAE loss: 6.858159 at the end of epoch by applying all training data 94\n",
      "For validation, mean MAE loss: 7.436543 at the end of epoch 94\n",
      "At 95-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.915616 during each epoch using batch data 95\n",
      "For training, mean MAE loss: 6.969397 at the end of epoch by applying all training data 95\n",
      "For validation, mean MAE loss: 7.427016 at the end of epoch 95\n",
      "At 96-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.791470 during each epoch using batch data 96\n",
      "For training, mean MAE loss: 6.775376 at the end of epoch by applying all training data 96\n",
      "For validation, mean MAE loss: 7.418095 at the end of epoch 96\n",
      "At 97-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.860012 during each epoch using batch data 97\n",
      "For training, mean MAE loss: 7.161160 at the end of epoch by applying all training data 97\n",
      "For validation, mean MAE loss: 7.414537 at the end of epoch 97\n",
      "At 98-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.689385 during each epoch using batch data 98\n",
      "For training, mean MAE loss: 6.868395 at the end of epoch by applying all training data 98\n",
      "For validation, mean MAE loss: 7.416728 at the end of epoch 98\n",
      "At 99-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.933417 during each epoch using batch data 99\n",
      "For training, mean MAE loss: 6.795003 at the end of epoch by applying all training data 99\n",
      "For validation, mean MAE loss: 7.420077 at the end of epoch 99\n",
      "At 100-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.991402 during each epoch using batch data 100\n",
      "For training, mean MAE loss: 6.945069 at the end of epoch by applying all training data 100\n",
      "For validation, mean MAE loss: 7.417429 at the end of epoch 100\n",
      "At 101-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.896497 during each epoch using batch data 101\n",
      "For training, mean MAE loss: 6.799061 at the end of epoch by applying all training data 101\n",
      "For validation, mean MAE loss: 7.408433 at the end of epoch 101\n",
      "At 102-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.759276 during each epoch using batch data 102\n",
      "For training, mean MAE loss: 6.711319 at the end of epoch by applying all training data 102\n",
      "For validation, mean MAE loss: 7.398086 at the end of epoch 102\n",
      "At 103-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.618752 during each epoch using batch data 103\n",
      "For training, mean MAE loss: 6.763463 at the end of epoch by applying all training data 103\n",
      "For validation, mean MAE loss: 7.394259 at the end of epoch 103\n",
      "At 104-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.818907 during each epoch using batch data 104\n",
      "For training, mean MAE loss: 6.748878 at the end of epoch by applying all training data 104\n",
      "For validation, mean MAE loss: 7.394921 at the end of epoch 104\n",
      "At 105-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.639137 during each epoch using batch data 105\n",
      "For training, mean MAE loss: 6.870669 at the end of epoch by applying all training data 105\n",
      "For validation, mean MAE loss: 7.403169 at the end of epoch 105\n",
      "At 106-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.819299 during each epoch using batch data 106\n",
      "For training, mean MAE loss: 7.028182 at the end of epoch by applying all training data 106\n",
      "For validation, mean MAE loss: 7.402484 at the end of epoch 106\n",
      "At 107-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.836232 during each epoch using batch data 107\n",
      "For training, mean MAE loss: 6.741961 at the end of epoch by applying all training data 107\n",
      "For validation, mean MAE loss: 7.391491 at the end of epoch 107\n",
      "At 108-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.762734 during each epoch using batch data 108\n",
      "For training, mean MAE loss: 6.819655 at the end of epoch by applying all training data 108\n",
      "For validation, mean MAE loss: 7.389842 at the end of epoch 108\n",
      "At 109-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.936941 during each epoch using batch data 109\n",
      "For training, mean MAE loss: 6.646489 at the end of epoch by applying all training data 109\n",
      "For validation, mean MAE loss: 7.386254 at the end of epoch 109\n",
      "At 110-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 7.144156 during each epoch using batch data 110\n",
      "For training, mean MAE loss: 6.838672 at the end of epoch by applying all training data 110\n",
      "For validation, mean MAE loss: 7.382106 at the end of epoch 110\n",
      "At 111-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.885828 during each epoch using batch data 111\n",
      "For training, mean MAE loss: 6.725678 at the end of epoch by applying all training data 111\n",
      "For validation, mean MAE loss: 7.384532 at the end of epoch 111\n",
      "At 112-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.639170 during each epoch using batch data 112\n",
      "For training, mean MAE loss: 6.806362 at the end of epoch by applying all training data 112\n",
      "For validation, mean MAE loss: 7.373660 at the end of epoch 112\n",
      "At 113-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.773614 during each epoch using batch data 113\n",
      "For training, mean MAE loss: 6.946398 at the end of epoch by applying all training data 113\n",
      "For validation, mean MAE loss: 7.369646 at the end of epoch 113\n",
      "At 114-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.717043 during each epoch using batch data 114\n",
      "For training, mean MAE loss: 6.791769 at the end of epoch by applying all training data 114\n",
      "For validation, mean MAE loss: 7.367266 at the end of epoch 114\n",
      "At 115-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.834374 during each epoch using batch data 115\n",
      "For training, mean MAE loss: 6.865610 at the end of epoch by applying all training data 115\n",
      "For validation, mean MAE loss: 7.364586 at the end of epoch 115\n",
      "At 116-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.671843 during each epoch using batch data 116\n",
      "For training, mean MAE loss: 6.585950 at the end of epoch by applying all training data 116\n",
      "For validation, mean MAE loss: 7.362103 at the end of epoch 116\n",
      "At 117-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.942440 during each epoch using batch data 117\n",
      "For training, mean MAE loss: 6.772241 at the end of epoch by applying all training data 117\n",
      "For validation, mean MAE loss: 7.369657 at the end of epoch 117\n",
      "At 118-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.817435 during each epoch using batch data 118\n",
      "For training, mean MAE loss: 6.797481 at the end of epoch by applying all training data 118\n",
      "For validation, mean MAE loss: 7.383476 at the end of epoch 118\n",
      "At 119-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.796350 during each epoch using batch data 119\n",
      "For training, mean MAE loss: 6.748699 at the end of epoch by applying all training data 119\n",
      "For validation, mean MAE loss: 7.382536 at the end of epoch 119\n",
      "At 120-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.856305 during each epoch using batch data 120\n",
      "For training, mean MAE loss: 6.882361 at the end of epoch by applying all training data 120\n",
      "For validation, mean MAE loss: 7.363450 at the end of epoch 120\n",
      "At 121-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.722151 during each epoch using batch data 121\n",
      "For training, mean MAE loss: 6.875750 at the end of epoch by applying all training data 121\n",
      "For validation, mean MAE loss: 7.349916 at the end of epoch 121\n",
      "At 122-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.771324 during each epoch using batch data 122\n",
      "For training, mean MAE loss: 6.797712 at the end of epoch by applying all training data 122\n",
      "For validation, mean MAE loss: 7.344349 at the end of epoch 122\n",
      "At 123-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.785134 during each epoch using batch data 123\n",
      "For training, mean MAE loss: 6.786118 at the end of epoch by applying all training data 123\n",
      "For validation, mean MAE loss: 7.344369 at the end of epoch 123\n",
      "At 124-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.682685 during each epoch using batch data 124\n",
      "For training, mean MAE loss: 6.712998 at the end of epoch by applying all training data 124\n",
      "For validation, mean MAE loss: 7.347070 at the end of epoch 124\n",
      "At 125-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.751599 during each epoch using batch data 125\n",
      "For training, mean MAE loss: 6.843857 at the end of epoch by applying all training data 125\n",
      "For validation, mean MAE loss: 7.341574 at the end of epoch 125\n",
      "At 126-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.709248 during each epoch using batch data 126\n",
      "For training, mean MAE loss: 6.954520 at the end of epoch by applying all training data 126\n",
      "For validation, mean MAE loss: 7.338262 at the end of epoch 126\n",
      "At 127-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.978088 during each epoch using batch data 127\n",
      "For training, mean MAE loss: 6.776568 at the end of epoch by applying all training data 127\n",
      "For validation, mean MAE loss: 7.329784 at the end of epoch 127\n",
      "At 128-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.808047 during each epoch using batch data 128\n",
      "For training, mean MAE loss: 6.971990 at the end of epoch by applying all training data 128\n",
      "For validation, mean MAE loss: 7.327845 at the end of epoch 128\n",
      "At 129-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.780014 during each epoch using batch data 129\n",
      "For training, mean MAE loss: 6.879575 at the end of epoch by applying all training data 129\n",
      "For validation, mean MAE loss: 7.325140 at the end of epoch 129\n",
      "At 130-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.806669 during each epoch using batch data 130\n",
      "For training, mean MAE loss: 6.661137 at the end of epoch by applying all training data 130\n",
      "For validation, mean MAE loss: 7.325143 at the end of epoch 130\n",
      "At 131-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.774251 during each epoch using batch data 131\n",
      "For training, mean MAE loss: 6.621228 at the end of epoch by applying all training data 131\n",
      "For validation, mean MAE loss: 7.323202 at the end of epoch 131\n",
      "At 132-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.872569 during each epoch using batch data 132\n",
      "For training, mean MAE loss: 6.827421 at the end of epoch by applying all training data 132\n",
      "For validation, mean MAE loss: 7.322174 at the end of epoch 132\n",
      "At 133-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.756752 during each epoch using batch data 133\n",
      "For training, mean MAE loss: 6.626023 at the end of epoch by applying all training data 133\n",
      "For validation, mean MAE loss: 7.324054 at the end of epoch 133\n",
      "At 134-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.816366 during each epoch using batch data 134\n",
      "For training, mean MAE loss: 6.764463 at the end of epoch by applying all training data 134\n",
      "For validation, mean MAE loss: 7.319635 at the end of epoch 134\n",
      "At 135-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.562275 during each epoch using batch data 135\n",
      "For training, mean MAE loss: 6.627076 at the end of epoch by applying all training data 135\n",
      "For validation, mean MAE loss: 7.327376 at the end of epoch 135\n",
      "At 136-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.842177 during each epoch using batch data 136\n",
      "For training, mean MAE loss: 6.663028 at the end of epoch by applying all training data 136\n",
      "For validation, mean MAE loss: 7.328914 at the end of epoch 136\n",
      "At 137-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.874421 during each epoch using batch data 137\n",
      "For training, mean MAE loss: 6.736037 at the end of epoch by applying all training data 137\n",
      "For validation, mean MAE loss: 7.325528 at the end of epoch 137\n",
      "At 138-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.678579 during each epoch using batch data 138\n",
      "For training, mean MAE loss: 6.681455 at the end of epoch by applying all training data 138\n",
      "For validation, mean MAE loss: 7.311863 at the end of epoch 138\n",
      "At 139-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.722438 during each epoch using batch data 139\n",
      "For training, mean MAE loss: 6.850744 at the end of epoch by applying all training data 139\n",
      "For validation, mean MAE loss: 7.304974 at the end of epoch 139\n",
      "At 140-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.767470 during each epoch using batch data 140\n",
      "For training, mean MAE loss: 6.931570 at the end of epoch by applying all training data 140\n",
      "For validation, mean MAE loss: 7.302608 at the end of epoch 140\n",
      "At 141-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.804388 during each epoch using batch data 141\n",
      "For training, mean MAE loss: 6.638566 at the end of epoch by applying all training data 141\n",
      "For validation, mean MAE loss: 7.296406 at the end of epoch 141\n",
      "At 142-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.813030 during each epoch using batch data 142\n",
      "For training, mean MAE loss: 6.663914 at the end of epoch by applying all training data 142\n",
      "For validation, mean MAE loss: 7.298812 at the end of epoch 142\n",
      "At 143-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.793169 during each epoch using batch data 143\n",
      "For training, mean MAE loss: 6.587052 at the end of epoch by applying all training data 143\n",
      "For validation, mean MAE loss: 7.305729 at the end of epoch 143\n",
      "At 144-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.846379 during each epoch using batch data 144\n",
      "For training, mean MAE loss: 6.800472 at the end of epoch by applying all training data 144\n",
      "For validation, mean MAE loss: 7.307183 at the end of epoch 144\n",
      "At 145-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.700586 during each epoch using batch data 145\n",
      "For training, mean MAE loss: 6.806675 at the end of epoch by applying all training data 145\n",
      "For validation, mean MAE loss: 7.290616 at the end of epoch 145\n",
      "At 146-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.833866 during each epoch using batch data 146\n",
      "For training, mean MAE loss: 6.873773 at the end of epoch by applying all training data 146\n",
      "For validation, mean MAE loss: 7.281473 at the end of epoch 146\n",
      "At 147-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.722057 during each epoch using batch data 147\n",
      "For training, mean MAE loss: 6.629351 at the end of epoch by applying all training data 147\n",
      "For validation, mean MAE loss: 7.284286 at the end of epoch 147\n",
      "At 148-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.602152 during each epoch using batch data 148\n",
      "For training, mean MAE loss: 6.736024 at the end of epoch by applying all training data 148\n",
      "For validation, mean MAE loss: 7.285949 at the end of epoch 148\n",
      "At 149-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.772177 during each epoch using batch data 149\n",
      "For training, mean MAE loss: 6.472320 at the end of epoch by applying all training data 149\n",
      "For validation, mean MAE loss: 7.290529 at the end of epoch 149\n",
      "At 150-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.782240 during each epoch using batch data 150\n",
      "For training, mean MAE loss: 6.458287 at the end of epoch by applying all training data 150\n",
      "For validation, mean MAE loss: 7.275927 at the end of epoch 150\n",
      "At 151-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.849769 during each epoch using batch data 151\n",
      "For training, mean MAE loss: 6.848992 at the end of epoch by applying all training data 151\n",
      "For validation, mean MAE loss: 7.269893 at the end of epoch 151\n",
      "At 152-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.719032 during each epoch using batch data 152\n",
      "For training, mean MAE loss: 6.760400 at the end of epoch by applying all training data 152\n",
      "For validation, mean MAE loss: 7.268665 at the end of epoch 152\n",
      "At 153-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.431756 during each epoch using batch data 153\n",
      "For training, mean MAE loss: 6.811023 at the end of epoch by applying all training data 153\n",
      "For validation, mean MAE loss: 7.267010 at the end of epoch 153\n",
      "At 154-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.645221 during each epoch using batch data 154\n",
      "For training, mean MAE loss: 6.574088 at the end of epoch by applying all training data 154\n",
      "For validation, mean MAE loss: 7.271283 at the end of epoch 154\n",
      "At 155-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.618236 during each epoch using batch data 155\n",
      "For training, mean MAE loss: 6.631189 at the end of epoch by applying all training data 155\n",
      "For validation, mean MAE loss: 7.280044 at the end of epoch 155\n",
      "At 156-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.633538 during each epoch using batch data 156\n",
      "For training, mean MAE loss: 6.699998 at the end of epoch by applying all training data 156\n",
      "For validation, mean MAE loss: 7.282359 at the end of epoch 156\n",
      "At 157-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.637679 during each epoch using batch data 157\n",
      "For training, mean MAE loss: 6.483260 at the end of epoch by applying all training data 157\n",
      "For validation, mean MAE loss: 7.268363 at the end of epoch 157\n",
      "At 158-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.637277 during each epoch using batch data 158\n",
      "For training, mean MAE loss: 6.718531 at the end of epoch by applying all training data 158\n",
      "For validation, mean MAE loss: 7.259872 at the end of epoch 158\n",
      "At 159-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.703497 during each epoch using batch data 159\n",
      "For training, mean MAE loss: 6.656764 at the end of epoch by applying all training data 159\n",
      "For validation, mean MAE loss: 7.260168 at the end of epoch 159\n",
      "At 160-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.553660 during each epoch using batch data 160\n",
      "For training, mean MAE loss: 6.512417 at the end of epoch by applying all training data 160\n",
      "For validation, mean MAE loss: 7.254223 at the end of epoch 160\n",
      "At 161-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.689895 during each epoch using batch data 161\n",
      "For training, mean MAE loss: 6.728695 at the end of epoch by applying all training data 161\n",
      "For validation, mean MAE loss: 7.250404 at the end of epoch 161\n",
      "At 162-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.884793 during each epoch using batch data 162\n",
      "For training, mean MAE loss: 6.554761 at the end of epoch by applying all training data 162\n",
      "For validation, mean MAE loss: 7.249752 at the end of epoch 162\n",
      "At 163-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.637057 during each epoch using batch data 163\n",
      "For training, mean MAE loss: 6.793370 at the end of epoch by applying all training data 163\n",
      "For validation, mean MAE loss: 7.245485 at the end of epoch 163\n",
      "At 164-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.541183 during each epoch using batch data 164\n",
      "For training, mean MAE loss: 6.456357 at the end of epoch by applying all training data 164\n",
      "For validation, mean MAE loss: 7.245952 at the end of epoch 164\n",
      "At 165-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.521226 during each epoch using batch data 165\n",
      "For training, mean MAE loss: 6.577580 at the end of epoch by applying all training data 165\n",
      "For validation, mean MAE loss: 7.256700 at the end of epoch 165\n",
      "At 166-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.798924 during each epoch using batch data 166\n",
      "For training, mean MAE loss: 6.644218 at the end of epoch by applying all training data 166\n",
      "For validation, mean MAE loss: 7.253288 at the end of epoch 166\n",
      "At 167-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.667721 during each epoch using batch data 167\n",
      "For training, mean MAE loss: 6.513688 at the end of epoch by applying all training data 167\n",
      "For validation, mean MAE loss: 7.236234 at the end of epoch 167\n",
      "At 168-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.782383 during each epoch using batch data 168\n",
      "For training, mean MAE loss: 6.713108 at the end of epoch by applying all training data 168\n",
      "For validation, mean MAE loss: 7.225932 at the end of epoch 168\n",
      "At 169-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.735187 during each epoch using batch data 169\n",
      "For training, mean MAE loss: 6.594322 at the end of epoch by applying all training data 169\n",
      "For validation, mean MAE loss: 7.224053 at the end of epoch 169\n",
      "At 170-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.852393 during each epoch using batch data 170\n",
      "For training, mean MAE loss: 6.671989 at the end of epoch by applying all training data 170\n",
      "For validation, mean MAE loss: 7.215342 at the end of epoch 170\n",
      "At 171-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.564257 during each epoch using batch data 171\n",
      "For training, mean MAE loss: 6.611095 at the end of epoch by applying all training data 171\n",
      "For validation, mean MAE loss: 7.207972 at the end of epoch 171\n",
      "At 172-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.642231 during each epoch using batch data 172\n",
      "For training, mean MAE loss: 6.791869 at the end of epoch by applying all training data 172\n",
      "For validation, mean MAE loss: 7.206764 at the end of epoch 172\n",
      "At 173-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.569004 during each epoch using batch data 173\n",
      "For training, mean MAE loss: 6.652095 at the end of epoch by applying all training data 173\n",
      "For validation, mean MAE loss: 7.206591 at the end of epoch 173\n",
      "At 174-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.503277 during each epoch using batch data 174\n",
      "For training, mean MAE loss: 6.810072 at the end of epoch by applying all training data 174\n",
      "For validation, mean MAE loss: 7.208670 at the end of epoch 174\n",
      "At 175-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.467341 during each epoch using batch data 175\n",
      "For training, mean MAE loss: 6.625856 at the end of epoch by applying all training data 175\n",
      "For validation, mean MAE loss: 7.209724 at the end of epoch 175\n",
      "At 176-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.470937 during each epoch using batch data 176\n",
      "For training, mean MAE loss: 6.411386 at the end of epoch by applying all training data 176\n",
      "For validation, mean MAE loss: 7.206471 at the end of epoch 176\n",
      "At 177-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.765023 during each epoch using batch data 177\n",
      "For training, mean MAE loss: 6.622916 at the end of epoch by applying all training data 177\n",
      "For validation, mean MAE loss: 7.207322 at the end of epoch 177\n",
      "At 178-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.501139 during each epoch using batch data 178\n",
      "For training, mean MAE loss: 6.622213 at the end of epoch by applying all training data 178\n",
      "For validation, mean MAE loss: 7.199245 at the end of epoch 178\n",
      "At 179-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.622646 during each epoch using batch data 179\n",
      "For training, mean MAE loss: 6.605626 at the end of epoch by applying all training data 179\n",
      "For validation, mean MAE loss: 7.208343 at the end of epoch 179\n",
      "At 180-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.585346 during each epoch using batch data 180\n",
      "For training, mean MAE loss: 6.690635 at the end of epoch by applying all training data 180\n",
      "For validation, mean MAE loss: 7.219802 at the end of epoch 180\n",
      "At 181-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.664693 during each epoch using batch data 181\n",
      "For training, mean MAE loss: 6.542163 at the end of epoch by applying all training data 181\n",
      "For validation, mean MAE loss: 7.204436 at the end of epoch 181\n",
      "At 182-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.699661 during each epoch using batch data 182\n",
      "For training, mean MAE loss: 6.749170 at the end of epoch by applying all training data 182\n",
      "For validation, mean MAE loss: 7.185474 at the end of epoch 182\n",
      "At 183-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.489288 during each epoch using batch data 183\n",
      "For training, mean MAE loss: 6.653830 at the end of epoch by applying all training data 183\n",
      "For validation, mean MAE loss: 7.176460 at the end of epoch 183\n",
      "At 184-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.652407 during each epoch using batch data 184\n",
      "For training, mean MAE loss: 6.465849 at the end of epoch by applying all training data 184\n",
      "For validation, mean MAE loss: 7.173816 at the end of epoch 184\n",
      "At 185-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.730748 during each epoch using batch data 185\n",
      "For training, mean MAE loss: 6.618744 at the end of epoch by applying all training data 185\n",
      "For validation, mean MAE loss: 7.191474 at the end of epoch 185\n",
      "At 186-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.578228 during each epoch using batch data 186\n",
      "For training, mean MAE loss: 6.570946 at the end of epoch by applying all training data 186\n",
      "For validation, mean MAE loss: 7.185894 at the end of epoch 186\n",
      "At 187-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.499875 during each epoch using batch data 187\n",
      "For training, mean MAE loss: 6.782679 at the end of epoch by applying all training data 187\n",
      "For validation, mean MAE loss: 7.177514 at the end of epoch 187\n",
      "At 188-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.784840 during each epoch using batch data 188\n",
      "For training, mean MAE loss: 6.522239 at the end of epoch by applying all training data 188\n",
      "For validation, mean MAE loss: 7.169955 at the end of epoch 188\n",
      "At 189-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.821306 during each epoch using batch data 189\n",
      "For training, mean MAE loss: 6.867336 at the end of epoch by applying all training data 189\n",
      "For validation, mean MAE loss: 7.170275 at the end of epoch 189\n",
      "At 190-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.559119 during each epoch using batch data 190\n",
      "For training, mean MAE loss: 6.675537 at the end of epoch by applying all training data 190\n",
      "For validation, mean MAE loss: 7.167412 at the end of epoch 190\n",
      "At 191-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.643040 during each epoch using batch data 191\n",
      "For training, mean MAE loss: 6.605842 at the end of epoch by applying all training data 191\n",
      "For validation, mean MAE loss: 7.170256 at the end of epoch 191\n",
      "At 192-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.454017 during each epoch using batch data 192\n",
      "For training, mean MAE loss: 6.362737 at the end of epoch by applying all training data 192\n",
      "For validation, mean MAE loss: 7.165101 at the end of epoch 192\n",
      "At 193-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.572730 during each epoch using batch data 193\n",
      "For training, mean MAE loss: 6.444263 at the end of epoch by applying all training data 193\n",
      "For validation, mean MAE loss: 7.163637 at the end of epoch 193\n",
      "At 194-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.466844 during each epoch using batch data 194\n",
      "For training, mean MAE loss: 6.565459 at the end of epoch by applying all training data 194\n",
      "For validation, mean MAE loss: 7.166292 at the end of epoch 194\n",
      "At 195-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.602721 during each epoch using batch data 195\n",
      "For training, mean MAE loss: 6.507697 at the end of epoch by applying all training data 195\n",
      "For validation, mean MAE loss: 7.153016 at the end of epoch 195\n",
      "At 196-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.889243 during each epoch using batch data 196\n",
      "For training, mean MAE loss: 6.329358 at the end of epoch by applying all training data 196\n",
      "For validation, mean MAE loss: 7.152471 at the end of epoch 196\n",
      "At 197-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.742647 during each epoch using batch data 197\n",
      "For training, mean MAE loss: 6.749994 at the end of epoch by applying all training data 197\n",
      "For validation, mean MAE loss: 7.144382 at the end of epoch 197\n",
      "At 198-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.570139 during each epoch using batch data 198\n",
      "For training, mean MAE loss: 6.508057 at the end of epoch by applying all training data 198\n",
      "For validation, mean MAE loss: 7.139609 at the end of epoch 198\n",
      "At 199-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.525209 during each epoch using batch data 199\n",
      "For training, mean MAE loss: 6.538289 at the end of epoch by applying all training data 199\n",
      "For validation, mean MAE loss: 7.151682 at the end of epoch 199\n",
      "At 200-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.590604 during each epoch using batch data 200\n",
      "For training, mean MAE loss: 6.462881 at the end of epoch by applying all training data 200\n",
      "For validation, mean MAE loss: 7.171252 at the end of epoch 200\n",
      "At 201-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.456980 during each epoch using batch data 201\n",
      "For training, mean MAE loss: 6.585386 at the end of epoch by applying all training data 201\n",
      "For validation, mean MAE loss: 7.147356 at the end of epoch 201\n",
      "At 202-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.516675 during each epoch using batch data 202\n",
      "For training, mean MAE loss: 6.374238 at the end of epoch by applying all training data 202\n",
      "For validation, mean MAE loss: 7.130191 at the end of epoch 202\n",
      "At 203-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.856686 during each epoch using batch data 203\n",
      "For training, mean MAE loss: 6.474577 at the end of epoch by applying all training data 203\n",
      "For validation, mean MAE loss: 7.118384 at the end of epoch 203\n",
      "At 204-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.504024 during each epoch using batch data 204\n",
      "For training, mean MAE loss: 6.704649 at the end of epoch by applying all training data 204\n",
      "For validation, mean MAE loss: 7.120530 at the end of epoch 204\n",
      "At 205-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.671175 during each epoch using batch data 205\n",
      "For training, mean MAE loss: 6.642217 at the end of epoch by applying all training data 205\n",
      "For validation, mean MAE loss: 7.118463 at the end of epoch 205\n",
      "At 206-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.544019 during each epoch using batch data 206\n",
      "For training, mean MAE loss: 6.533211 at the end of epoch by applying all training data 206\n",
      "For validation, mean MAE loss: 7.109909 at the end of epoch 206\n",
      "At 207-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.778939 during each epoch using batch data 207\n",
      "For training, mean MAE loss: 6.423622 at the end of epoch by applying all training data 207\n",
      "For validation, mean MAE loss: 7.112154 at the end of epoch 207\n",
      "At 208-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.647012 during each epoch using batch data 208\n",
      "For training, mean MAE loss: 6.484960 at the end of epoch by applying all training data 208\n",
      "For validation, mean MAE loss: 7.115410 at the end of epoch 208\n",
      "At 209-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.530367 during each epoch using batch data 209\n",
      "For training, mean MAE loss: 6.285164 at the end of epoch by applying all training data 209\n",
      "For validation, mean MAE loss: 7.116830 at the end of epoch 209\n",
      "At 210-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.436693 during each epoch using batch data 210\n",
      "For training, mean MAE loss: 6.550626 at the end of epoch by applying all training data 210\n",
      "For validation, mean MAE loss: 7.114879 at the end of epoch 210\n",
      "At 211-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.547148 during each epoch using batch data 211\n",
      "For training, mean MAE loss: 6.357174 at the end of epoch by applying all training data 211\n",
      "For validation, mean MAE loss: 7.097706 at the end of epoch 211\n",
      "At 212-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.441261 during each epoch using batch data 212\n",
      "For training, mean MAE loss: 6.640578 at the end of epoch by applying all training data 212\n",
      "For validation, mean MAE loss: 7.098175 at the end of epoch 212\n",
      "At 213-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.607582 during each epoch using batch data 213\n",
      "For training, mean MAE loss: 6.465166 at the end of epoch by applying all training data 213\n",
      "For validation, mean MAE loss: 7.094372 at the end of epoch 213\n",
      "At 214-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.441188 during each epoch using batch data 214\n",
      "For training, mean MAE loss: 6.405967 at the end of epoch by applying all training data 214\n",
      "For validation, mean MAE loss: 7.095762 at the end of epoch 214\n",
      "At 215-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.791772 during each epoch using batch data 215\n",
      "For training, mean MAE loss: 6.495265 at the end of epoch by applying all training data 215\n",
      "For validation, mean MAE loss: 7.088804 at the end of epoch 215\n",
      "At 216-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.330279 during each epoch using batch data 216\n",
      "For training, mean MAE loss: 6.402954 at the end of epoch by applying all training data 216\n",
      "For validation, mean MAE loss: 7.086487 at the end of epoch 216\n",
      "At 217-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.425117 during each epoch using batch data 217\n",
      "For training, mean MAE loss: 6.625104 at the end of epoch by applying all training data 217\n",
      "For validation, mean MAE loss: 7.091014 at the end of epoch 217\n",
      "At 218-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.382234 during each epoch using batch data 218\n",
      "For training, mean MAE loss: 6.325204 at the end of epoch by applying all training data 218\n",
      "For validation, mean MAE loss: 7.087303 at the end of epoch 218\n",
      "At 219-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.750548 during each epoch using batch data 219\n",
      "For training, mean MAE loss: 6.502246 at the end of epoch by applying all training data 219\n",
      "For validation, mean MAE loss: 7.086748 at the end of epoch 219\n",
      "At 220-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.601017 during each epoch using batch data 220\n",
      "For training, mean MAE loss: 6.442183 at the end of epoch by applying all training data 220\n",
      "For validation, mean MAE loss: 7.071843 at the end of epoch 220\n",
      "At 221-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.439678 during each epoch using batch data 221\n",
      "For training, mean MAE loss: 6.626283 at the end of epoch by applying all training data 221\n",
      "For validation, mean MAE loss: 7.064312 at the end of epoch 221\n",
      "At 222-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.807158 during each epoch using batch data 222\n",
      "For training, mean MAE loss: 6.289171 at the end of epoch by applying all training data 222\n",
      "For validation, mean MAE loss: 7.067265 at the end of epoch 222\n",
      "At 223-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.356460 during each epoch using batch data 223\n",
      "For training, mean MAE loss: 6.437947 at the end of epoch by applying all training data 223\n",
      "For validation, mean MAE loss: 7.069594 at the end of epoch 223\n",
      "At 224-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.408172 during each epoch using batch data 224\n",
      "For training, mean MAE loss: 6.351277 at the end of epoch by applying all training data 224\n",
      "For validation, mean MAE loss: 7.077775 at the end of epoch 224\n",
      "At 225-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.382133 during each epoch using batch data 225\n",
      "For training, mean MAE loss: 6.334070 at the end of epoch by applying all training data 225\n",
      "For validation, mean MAE loss: 7.065277 at the end of epoch 225\n",
      "At 226-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.607597 during each epoch using batch data 226\n",
      "For training, mean MAE loss: 6.585618 at the end of epoch by applying all training data 226\n",
      "For validation, mean MAE loss: 7.056094 at the end of epoch 226\n",
      "At 227-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.534610 during each epoch using batch data 227\n",
      "For training, mean MAE loss: 6.517763 at the end of epoch by applying all training data 227\n",
      "For validation, mean MAE loss: 7.050211 at the end of epoch 227\n",
      "At 228-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.442971 during each epoch using batch data 228\n",
      "For training, mean MAE loss: 6.363743 at the end of epoch by applying all training data 228\n",
      "For validation, mean MAE loss: 7.049006 at the end of epoch 228\n",
      "At 229-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.340958 during each epoch using batch data 229\n",
      "For training, mean MAE loss: 6.330231 at the end of epoch by applying all training data 229\n",
      "For validation, mean MAE loss: 7.051284 at the end of epoch 229\n",
      "At 230-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.217683 during each epoch using batch data 230\n",
      "For training, mean MAE loss: 6.428713 at the end of epoch by applying all training data 230\n",
      "For validation, mean MAE loss: 7.047790 at the end of epoch 230\n",
      "At 231-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.301071 during each epoch using batch data 231\n",
      "For training, mean MAE loss: 6.472256 at the end of epoch by applying all training data 231\n",
      "For validation, mean MAE loss: 7.044984 at the end of epoch 231\n",
      "At 232-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.551559 during each epoch using batch data 232\n",
      "For training, mean MAE loss: 6.474012 at the end of epoch by applying all training data 232\n",
      "For validation, mean MAE loss: 7.046607 at the end of epoch 232\n",
      "At 233-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.412251 during each epoch using batch data 233\n",
      "For training, mean MAE loss: 6.378805 at the end of epoch by applying all training data 233\n",
      "For validation, mean MAE loss: 7.031634 at the end of epoch 233\n",
      "At 234-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.385596 during each epoch using batch data 234\n",
      "For training, mean MAE loss: 6.327832 at the end of epoch by applying all training data 234\n",
      "For validation, mean MAE loss: 7.028514 at the end of epoch 234\n",
      "At 235-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.831311 during each epoch using batch data 235\n",
      "For training, mean MAE loss: 6.499539 at the end of epoch by applying all training data 235\n",
      "For validation, mean MAE loss: 7.025234 at the end of epoch 235\n",
      "At 236-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.445229 during each epoch using batch data 236\n",
      "For training, mean MAE loss: 6.525698 at the end of epoch by applying all training data 236\n",
      "For validation, mean MAE loss: 7.028126 at the end of epoch 236\n",
      "At 237-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.675496 during each epoch using batch data 237\n",
      "For training, mean MAE loss: 6.414359 at the end of epoch by applying all training data 237\n",
      "For validation, mean MAE loss: 7.050964 at the end of epoch 237\n",
      "At 238-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.431785 during each epoch using batch data 238\n",
      "For training, mean MAE loss: 6.536519 at the end of epoch by applying all training data 238\n",
      "For validation, mean MAE loss: 7.028084 at the end of epoch 238\n",
      "At 239-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.767604 during each epoch using batch data 239\n",
      "For training, mean MAE loss: 6.496342 at the end of epoch by applying all training data 239\n",
      "For validation, mean MAE loss: 7.015318 at the end of epoch 239\n",
      "At 240-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.296327 during each epoch using batch data 240\n",
      "For training, mean MAE loss: 6.359461 at the end of epoch by applying all training data 240\n",
      "For validation, mean MAE loss: 7.011733 at the end of epoch 240\n",
      "At 241-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.321696 during each epoch using batch data 241\n",
      "For training, mean MAE loss: 6.534176 at the end of epoch by applying all training data 241\n",
      "For validation, mean MAE loss: 7.011254 at the end of epoch 241\n",
      "At 242-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.575024 during each epoch using batch data 242\n",
      "For training, mean MAE loss: 6.477216 at the end of epoch by applying all training data 242\n",
      "For validation, mean MAE loss: 7.012326 at the end of epoch 242\n",
      "At 243-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.509857 during each epoch using batch data 243\n",
      "For training, mean MAE loss: 6.434055 at the end of epoch by applying all training data 243\n",
      "For validation, mean MAE loss: 7.005556 at the end of epoch 243\n",
      "At 244-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.642988 during each epoch using batch data 244\n",
      "For training, mean MAE loss: 6.565488 at the end of epoch by applying all training data 244\n",
      "For validation, mean MAE loss: 7.002836 at the end of epoch 244\n",
      "At 245-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.392762 during each epoch using batch data 245\n",
      "For training, mean MAE loss: 6.262126 at the end of epoch by applying all training data 245\n",
      "For validation, mean MAE loss: 6.999180 at the end of epoch 245\n",
      "At 246-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.665092 during each epoch using batch data 246\n",
      "For training, mean MAE loss: 6.462699 at the end of epoch by applying all training data 246\n",
      "For validation, mean MAE loss: 6.995744 at the end of epoch 246\n",
      "At 247-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.365872 during each epoch using batch data 247\n",
      "For training, mean MAE loss: 6.405994 at the end of epoch by applying all training data 247\n",
      "For validation, mean MAE loss: 6.990688 at the end of epoch 247\n",
      "At 248-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.388170 during each epoch using batch data 248\n",
      "For training, mean MAE loss: 6.405898 at the end of epoch by applying all training data 248\n",
      "For validation, mean MAE loss: 6.987606 at the end of epoch 248\n",
      "At 249-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.436468 during each epoch using batch data 249\n",
      "For training, mean MAE loss: 6.536250 at the end of epoch by applying all training data 249\n",
      "For validation, mean MAE loss: 6.984419 at the end of epoch 249\n",
      "At 250-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.344173 during each epoch using batch data 250\n",
      "For training, mean MAE loss: 6.468572 at the end of epoch by applying all training data 250\n",
      "For validation, mean MAE loss: 6.983523 at the end of epoch 250\n",
      "At 251-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.314065 during each epoch using batch data 251\n",
      "For training, mean MAE loss: 6.592773 at the end of epoch by applying all training data 251\n",
      "For validation, mean MAE loss: 6.985345 at the end of epoch 251\n",
      "At 252-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.206341 during each epoch using batch data 252\n",
      "For training, mean MAE loss: 6.525744 at the end of epoch by applying all training data 252\n",
      "For validation, mean MAE loss: 6.982578 at the end of epoch 252\n",
      "At 253-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.372600 during each epoch using batch data 253\n",
      "For training, mean MAE loss: 6.369736 at the end of epoch by applying all training data 253\n",
      "For validation, mean MAE loss: 6.977442 at the end of epoch 253\n",
      "At 254-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.398051 during each epoch using batch data 254\n",
      "For training, mean MAE loss: 6.416844 at the end of epoch by applying all training data 254\n",
      "For validation, mean MAE loss: 6.971181 at the end of epoch 254\n",
      "At 255-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.426065 during each epoch using batch data 255\n",
      "For training, mean MAE loss: 6.678996 at the end of epoch by applying all training data 255\n",
      "For validation, mean MAE loss: 6.968422 at the end of epoch 255\n",
      "At 256-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.290678 during each epoch using batch data 256\n",
      "For training, mean MAE loss: 6.488283 at the end of epoch by applying all training data 256\n",
      "For validation, mean MAE loss: 6.965947 at the end of epoch 256\n",
      "At 257-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.596610 during each epoch using batch data 257\n",
      "For training, mean MAE loss: 6.466762 at the end of epoch by applying all training data 257\n",
      "For validation, mean MAE loss: 6.964483 at the end of epoch 257\n",
      "At 258-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.154118 during each epoch using batch data 258\n",
      "For training, mean MAE loss: 6.319083 at the end of epoch by applying all training data 258\n",
      "For validation, mean MAE loss: 6.960367 at the end of epoch 258\n",
      "At 259-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.355345 during each epoch using batch data 259\n",
      "For training, mean MAE loss: 6.208836 at the end of epoch by applying all training data 259\n",
      "For validation, mean MAE loss: 6.957741 at the end of epoch 259\n",
      "At 260-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.306109 during each epoch using batch data 260\n",
      "For training, mean MAE loss: 6.211367 at the end of epoch by applying all training data 260\n",
      "For validation, mean MAE loss: 6.954547 at the end of epoch 260\n",
      "At 261-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.475810 during each epoch using batch data 261\n",
      "For training, mean MAE loss: 6.302574 at the end of epoch by applying all training data 261\n",
      "For validation, mean MAE loss: 6.951602 at the end of epoch 261\n",
      "At 262-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.369007 during each epoch using batch data 262\n",
      "For training, mean MAE loss: 6.436684 at the end of epoch by applying all training data 262\n",
      "For validation, mean MAE loss: 6.962936 at the end of epoch 262\n",
      "At 263-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.202126 during each epoch using batch data 263\n",
      "For training, mean MAE loss: 6.263814 at the end of epoch by applying all training data 263\n",
      "For validation, mean MAE loss: 6.948283 at the end of epoch 263\n",
      "At 264-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.444685 during each epoch using batch data 264\n",
      "For training, mean MAE loss: 6.471329 at the end of epoch by applying all training data 264\n",
      "For validation, mean MAE loss: 6.942729 at the end of epoch 264\n",
      "At 265-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.344040 during each epoch using batch data 265\n",
      "For training, mean MAE loss: 6.256921 at the end of epoch by applying all training data 265\n",
      "For validation, mean MAE loss: 6.949308 at the end of epoch 265\n",
      "At 266-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.386698 during each epoch using batch data 266\n",
      "For training, mean MAE loss: 6.338410 at the end of epoch by applying all training data 266\n",
      "For validation, mean MAE loss: 6.946520 at the end of epoch 266\n",
      "At 267-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.225029 during each epoch using batch data 267\n",
      "For training, mean MAE loss: 6.111618 at the end of epoch by applying all training data 267\n",
      "For validation, mean MAE loss: 6.941670 at the end of epoch 267\n",
      "At 268-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.197486 during each epoch using batch data 268\n",
      "For training, mean MAE loss: 6.307509 at the end of epoch by applying all training data 268\n",
      "For validation, mean MAE loss: 6.935067 at the end of epoch 268\n",
      "At 269-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.239244 during each epoch using batch data 269\n",
      "For training, mean MAE loss: 6.289266 at the end of epoch by applying all training data 269\n",
      "For validation, mean MAE loss: 6.931100 at the end of epoch 269\n",
      "At 270-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.355953 during each epoch using batch data 270\n",
      "For training, mean MAE loss: 6.276276 at the end of epoch by applying all training data 270\n",
      "For validation, mean MAE loss: 6.924974 at the end of epoch 270\n",
      "At 271-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.347912 during each epoch using batch data 271\n",
      "For training, mean MAE loss: 6.310145 at the end of epoch by applying all training data 271\n",
      "For validation, mean MAE loss: 6.921316 at the end of epoch 271\n",
      "At 272-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.220644 during each epoch using batch data 272\n",
      "For training, mean MAE loss: 6.404832 at the end of epoch by applying all training data 272\n",
      "For validation, mean MAE loss: 6.918438 at the end of epoch 272\n",
      "At 273-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.321451 during each epoch using batch data 273\n",
      "For training, mean MAE loss: 6.375358 at the end of epoch by applying all training data 273\n",
      "For validation, mean MAE loss: 6.915662 at the end of epoch 273\n",
      "At 274-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.233689 during each epoch using batch data 274\n",
      "For training, mean MAE loss: 6.454053 at the end of epoch by applying all training data 274\n",
      "For validation, mean MAE loss: 6.922056 at the end of epoch 274\n",
      "At 275-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.402911 during each epoch using batch data 275\n",
      "For training, mean MAE loss: 6.306812 at the end of epoch by applying all training data 275\n",
      "For validation, mean MAE loss: 6.949174 at the end of epoch 275\n",
      "At 276-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.264664 during each epoch using batch data 276\n",
      "For training, mean MAE loss: 6.315329 at the end of epoch by applying all training data 276\n",
      "For validation, mean MAE loss: 6.917092 at the end of epoch 276\n",
      "At 277-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.496787 during each epoch using batch data 277\n",
      "For training, mean MAE loss: 6.596442 at the end of epoch by applying all training data 277\n",
      "For validation, mean MAE loss: 6.925950 at the end of epoch 277\n",
      "At 278-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.315450 during each epoch using batch data 278\n",
      "For training, mean MAE loss: 6.134075 at the end of epoch by applying all training data 278\n",
      "For validation, mean MAE loss: 6.921415 at the end of epoch 278\n",
      "At 279-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.193698 during each epoch using batch data 279\n",
      "For training, mean MAE loss: 6.223412 at the end of epoch by applying all training data 279\n",
      "For validation, mean MAE loss: 6.909244 at the end of epoch 279\n",
      "At 280-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.599920 during each epoch using batch data 280\n",
      "For training, mean MAE loss: 6.492470 at the end of epoch by applying all training data 280\n",
      "For validation, mean MAE loss: 6.903564 at the end of epoch 280\n",
      "At 281-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.166721 during each epoch using batch data 281\n",
      "For training, mean MAE loss: 6.147044 at the end of epoch by applying all training data 281\n",
      "For validation, mean MAE loss: 6.916190 at the end of epoch 281\n",
      "At 282-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.297972 during each epoch using batch data 282\n",
      "For training, mean MAE loss: 6.108466 at the end of epoch by applying all training data 282\n",
      "For validation, mean MAE loss: 6.902123 at the end of epoch 282\n",
      "At 283-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.274617 during each epoch using batch data 283\n",
      "For training, mean MAE loss: 6.172509 at the end of epoch by applying all training data 283\n",
      "For validation, mean MAE loss: 6.897814 at the end of epoch 283\n",
      "At 284-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.165486 during each epoch using batch data 284\n",
      "For training, mean MAE loss: 6.438976 at the end of epoch by applying all training data 284\n",
      "For validation, mean MAE loss: 6.923096 at the end of epoch 284\n",
      "At 285-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.331179 during each epoch using batch data 285\n",
      "For training, mean MAE loss: 6.453483 at the end of epoch by applying all training data 285\n",
      "For validation, mean MAE loss: 6.921627 at the end of epoch 285\n",
      "At 286-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.396690 during each epoch using batch data 286\n",
      "For training, mean MAE loss: 6.175983 at the end of epoch by applying all training data 286\n",
      "For validation, mean MAE loss: 6.900403 at the end of epoch 286\n",
      "At 287-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.363816 during each epoch using batch data 287\n",
      "For training, mean MAE loss: 6.311714 at the end of epoch by applying all training data 287\n",
      "For validation, mean MAE loss: 6.909252 at the end of epoch 287\n",
      "At 288-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.116675 during each epoch using batch data 288\n",
      "For training, mean MAE loss: 6.305978 at the end of epoch by applying all training data 288\n",
      "For validation, mean MAE loss: 6.887534 at the end of epoch 288\n",
      "At 289-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.391278 during each epoch using batch data 289\n",
      "For training, mean MAE loss: 6.189143 at the end of epoch by applying all training data 289\n",
      "For validation, mean MAE loss: 6.879426 at the end of epoch 289\n",
      "At 290-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.107473 during each epoch using batch data 290\n",
      "For training, mean MAE loss: 6.213071 at the end of epoch by applying all training data 290\n",
      "For validation, mean MAE loss: 6.907891 at the end of epoch 290\n",
      "At 291-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.197438 during each epoch using batch data 291\n",
      "For training, mean MAE loss: 6.156966 at the end of epoch by applying all training data 291\n",
      "For validation, mean MAE loss: 6.912791 at the end of epoch 291\n",
      "At 292-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.258132 during each epoch using batch data 292\n",
      "For training, mean MAE loss: 6.236030 at the end of epoch by applying all training data 292\n",
      "For validation, mean MAE loss: 6.897839 at the end of epoch 292\n",
      "At 293-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.092109 during each epoch using batch data 293\n",
      "For training, mean MAE loss: 6.308250 at the end of epoch by applying all training data 293\n",
      "For validation, mean MAE loss: 6.899046 at the end of epoch 293\n",
      "At 294-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.260243 during each epoch using batch data 294\n",
      "For training, mean MAE loss: 6.246027 at the end of epoch by applying all training data 294\n",
      "For validation, mean MAE loss: 6.877931 at the end of epoch 294\n",
      "At 295-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.341693 during each epoch using batch data 295\n",
      "For training, mean MAE loss: 6.255085 at the end of epoch by applying all training data 295\n",
      "For validation, mean MAE loss: 6.869285 at the end of epoch 295\n",
      "At 296-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.424618 during each epoch using batch data 296\n",
      "For training, mean MAE loss: 6.345462 at the end of epoch by applying all training data 296\n",
      "For validation, mean MAE loss: 6.886466 at the end of epoch 296\n",
      "At 297-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.258795 during each epoch using batch data 297\n",
      "For training, mean MAE loss: 6.309289 at the end of epoch by applying all training data 297\n",
      "For validation, mean MAE loss: 6.870050 at the end of epoch 297\n",
      "At 298-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.159287 during each epoch using batch data 298\n",
      "For training, mean MAE loss: 6.296762 at the end of epoch by applying all training data 298\n",
      "For validation, mean MAE loss: 6.866148 at the end of epoch 298\n",
      "At 299-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.134033 during each epoch using batch data 299\n",
      "For training, mean MAE loss: 6.111369 at the end of epoch by applying all training data 299\n",
      "For validation, mean MAE loss: 6.883453 at the end of epoch 299\n",
      "At 300-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.367643 during each epoch using batch data 300\n",
      "For training, mean MAE loss: 6.411035 at the end of epoch by applying all training data 300\n",
      "For validation, mean MAE loss: 6.872541 at the end of epoch 300\n",
      "At 301-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.121159 during each epoch using batch data 301\n",
      "For training, mean MAE loss: 6.113206 at the end of epoch by applying all training data 301\n",
      "For validation, mean MAE loss: 6.869832 at the end of epoch 301\n",
      "At 302-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.273177 during each epoch using batch data 302\n",
      "For training, mean MAE loss: 6.107703 at the end of epoch by applying all training data 302\n",
      "For validation, mean MAE loss: 6.851778 at the end of epoch 302\n",
      "At 303-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.020465 during each epoch using batch data 303\n",
      "For training, mean MAE loss: 6.331528 at the end of epoch by applying all training data 303\n",
      "For validation, mean MAE loss: 6.847666 at the end of epoch 303\n",
      "At 304-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.201317 during each epoch using batch data 304\n",
      "For training, mean MAE loss: 6.418556 at the end of epoch by applying all training data 304\n",
      "For validation, mean MAE loss: 6.870922 at the end of epoch 304\n",
      "At 305-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.283612 during each epoch using batch data 305\n",
      "For training, mean MAE loss: 6.153064 at the end of epoch by applying all training data 305\n",
      "For validation, mean MAE loss: 6.865993 at the end of epoch 305\n",
      "At 306-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.201689 during each epoch using batch data 306\n",
      "For training, mean MAE loss: 6.236747 at the end of epoch by applying all training data 306\n",
      "For validation, mean MAE loss: 6.850384 at the end of epoch 306\n",
      "At 307-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.174478 during each epoch using batch data 307\n",
      "For training, mean MAE loss: 6.223022 at the end of epoch by applying all training data 307\n",
      "For validation, mean MAE loss: 6.865541 at the end of epoch 307\n",
      "At 308-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.295589 during each epoch using batch data 308\n",
      "For training, mean MAE loss: 6.120858 at the end of epoch by applying all training data 308\n",
      "For validation, mean MAE loss: 6.853005 at the end of epoch 308\n",
      "At 309-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.330945 during each epoch using batch data 309\n",
      "For training, mean MAE loss: 6.242474 at the end of epoch by applying all training data 309\n",
      "For validation, mean MAE loss: 6.845865 at the end of epoch 309\n",
      "At 310-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.271049 during each epoch using batch data 310\n",
      "For training, mean MAE loss: 6.333552 at the end of epoch by applying all training data 310\n",
      "For validation, mean MAE loss: 6.838933 at the end of epoch 310\n",
      "At 311-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.219043 during each epoch using batch data 311\n",
      "For training, mean MAE loss: 6.037411 at the end of epoch by applying all training data 311\n",
      "For validation, mean MAE loss: 6.854524 at the end of epoch 311\n",
      "At 312-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.404235 during each epoch using batch data 312\n",
      "For training, mean MAE loss: 6.173486 at the end of epoch by applying all training data 312\n",
      "For validation, mean MAE loss: 6.852801 at the end of epoch 312\n",
      "At 313-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.222126 during each epoch using batch data 313\n",
      "For training, mean MAE loss: 6.223616 at the end of epoch by applying all training data 313\n",
      "For validation, mean MAE loss: 6.845039 at the end of epoch 313\n",
      "At 314-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.367388 during each epoch using batch data 314\n",
      "For training, mean MAE loss: 6.205509 at the end of epoch by applying all training data 314\n",
      "For validation, mean MAE loss: 6.859177 at the end of epoch 314\n",
      "At 315-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.281603 during each epoch using batch data 315\n",
      "For training, mean MAE loss: 6.214437 at the end of epoch by applying all training data 315\n",
      "For validation, mean MAE loss: 6.848002 at the end of epoch 315\n",
      "At 316-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.124104 during each epoch using batch data 316\n",
      "For training, mean MAE loss: 6.272693 at the end of epoch by applying all training data 316\n",
      "For validation, mean MAE loss: 6.833806 at the end of epoch 316\n",
      "At 317-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.166589 during each epoch using batch data 317\n",
      "For training, mean MAE loss: 6.046120 at the end of epoch by applying all training data 317\n",
      "For validation, mean MAE loss: 6.827849 at the end of epoch 317\n",
      "At 318-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.359964 during each epoch using batch data 318\n",
      "For training, mean MAE loss: 6.169366 at the end of epoch by applying all training data 318\n",
      "For validation, mean MAE loss: 6.815605 at the end of epoch 318\n",
      "At 319-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.171045 during each epoch using batch data 319\n",
      "For training, mean MAE loss: 6.161142 at the end of epoch by applying all training data 319\n",
      "For validation, mean MAE loss: 6.839934 at the end of epoch 319\n",
      "At 320-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.271361 during each epoch using batch data 320\n",
      "For training, mean MAE loss: 6.011904 at the end of epoch by applying all training data 320\n",
      "For validation, mean MAE loss: 6.851678 at the end of epoch 320\n",
      "At 321-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.426074 during each epoch using batch data 321\n",
      "For training, mean MAE loss: 6.365175 at the end of epoch by applying all training data 321\n",
      "For validation, mean MAE loss: 6.850165 at the end of epoch 321\n",
      "At 322-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.121180 during each epoch using batch data 322\n",
      "For training, mean MAE loss: 6.404165 at the end of epoch by applying all training data 322\n",
      "For validation, mean MAE loss: 6.824463 at the end of epoch 322\n",
      "At 323-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.299905 during each epoch using batch data 323\n",
      "For training, mean MAE loss: 6.102950 at the end of epoch by applying all training data 323\n",
      "For validation, mean MAE loss: 6.814175 at the end of epoch 323\n",
      "At 324-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.289897 during each epoch using batch data 324\n",
      "For training, mean MAE loss: 5.970805 at the end of epoch by applying all training data 324\n",
      "For validation, mean MAE loss: 6.826276 at the end of epoch 324\n",
      "At 325-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.418351 during each epoch using batch data 325\n",
      "For training, mean MAE loss: 6.231962 at the end of epoch by applying all training data 325\n",
      "For validation, mean MAE loss: 6.826640 at the end of epoch 325\n",
      "At 326-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.327258 during each epoch using batch data 326\n",
      "For training, mean MAE loss: 6.148211 at the end of epoch by applying all training data 326\n",
      "For validation, mean MAE loss: 6.798820 at the end of epoch 326\n",
      "At 327-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.233818 during each epoch using batch data 327\n",
      "For training, mean MAE loss: 6.317030 at the end of epoch by applying all training data 327\n",
      "For validation, mean MAE loss: 6.795830 at the end of epoch 327\n",
      "At 328-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.420495 during each epoch using batch data 328\n",
      "For training, mean MAE loss: 6.031328 at the end of epoch by applying all training data 328\n",
      "For validation, mean MAE loss: 6.815271 at the end of epoch 328\n",
      "At 329-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.118982 during each epoch using batch data 329\n",
      "For training, mean MAE loss: 6.320418 at the end of epoch by applying all training data 329\n",
      "For validation, mean MAE loss: 6.822065 at the end of epoch 329\n",
      "At 330-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.290585 during each epoch using batch data 330\n",
      "For training, mean MAE loss: 6.245756 at the end of epoch by applying all training data 330\n",
      "For validation, mean MAE loss: 6.817616 at the end of epoch 330\n",
      "At 331-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.367828 during each epoch using batch data 331\n",
      "For training, mean MAE loss: 6.098665 at the end of epoch by applying all training data 331\n",
      "For validation, mean MAE loss: 6.802845 at the end of epoch 331\n",
      "At 332-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.906859 during each epoch using batch data 332\n",
      "For training, mean MAE loss: 6.359377 at the end of epoch by applying all training data 332\n",
      "For validation, mean MAE loss: 6.800211 at the end of epoch 332\n",
      "At 333-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.139755 during each epoch using batch data 333\n",
      "For training, mean MAE loss: 6.222247 at the end of epoch by applying all training data 333\n",
      "For validation, mean MAE loss: 6.799588 at the end of epoch 333\n",
      "At 334-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.122632 during each epoch using batch data 334\n",
      "For training, mean MAE loss: 6.127582 at the end of epoch by applying all training data 334\n",
      "For validation, mean MAE loss: 6.808790 at the end of epoch 334\n",
      "At 335-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.994696 during each epoch using batch data 335\n",
      "For training, mean MAE loss: 6.038579 at the end of epoch by applying all training data 335\n",
      "For validation, mean MAE loss: 6.792057 at the end of epoch 335\n",
      "At 336-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.140578 during each epoch using batch data 336\n",
      "For training, mean MAE loss: 6.250968 at the end of epoch by applying all training data 336\n",
      "For validation, mean MAE loss: 6.791849 at the end of epoch 336\n",
      "At 337-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.010466 during each epoch using batch data 337\n",
      "For training, mean MAE loss: 6.341624 at the end of epoch by applying all training data 337\n",
      "For validation, mean MAE loss: 6.785983 at the end of epoch 337\n",
      "At 338-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.301513 during each epoch using batch data 338\n",
      "For training, mean MAE loss: 6.145612 at the end of epoch by applying all training data 338\n",
      "For validation, mean MAE loss: 6.781502 at the end of epoch 338\n",
      "At 339-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.066507 during each epoch using batch data 339\n",
      "For training, mean MAE loss: 6.155734 at the end of epoch by applying all training data 339\n",
      "For validation, mean MAE loss: 6.821315 at the end of epoch 339\n",
      "At 340-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.125816 during each epoch using batch data 340\n",
      "For training, mean MAE loss: 6.098710 at the end of epoch by applying all training data 340\n",
      "For validation, mean MAE loss: 6.800632 at the end of epoch 340\n",
      "At 341-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.164841 during each epoch using batch data 341\n",
      "For training, mean MAE loss: 6.315558 at the end of epoch by applying all training data 341\n",
      "For validation, mean MAE loss: 6.772545 at the end of epoch 341\n",
      "At 342-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.112379 during each epoch using batch data 342\n",
      "For training, mean MAE loss: 6.180699 at the end of epoch by applying all training data 342\n",
      "For validation, mean MAE loss: 6.765278 at the end of epoch 342\n",
      "At 343-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.217753 during each epoch using batch data 343\n",
      "For training, mean MAE loss: 6.221908 at the end of epoch by applying all training data 343\n",
      "For validation, mean MAE loss: 6.775365 at the end of epoch 343\n",
      "At 344-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.422426 during each epoch using batch data 344\n",
      "For training, mean MAE loss: 6.116572 at the end of epoch by applying all training data 344\n",
      "For validation, mean MAE loss: 6.785045 at the end of epoch 344\n",
      "At 345-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.216710 during each epoch using batch data 345\n",
      "For training, mean MAE loss: 6.247032 at the end of epoch by applying all training data 345\n",
      "For validation, mean MAE loss: 6.783279 at the end of epoch 345\n",
      "At 346-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.028755 during each epoch using batch data 346\n",
      "For training, mean MAE loss: 6.296496 at the end of epoch by applying all training data 346\n",
      "For validation, mean MAE loss: 6.786167 at the end of epoch 346\n",
      "At 347-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.110753 during each epoch using batch data 347\n",
      "For training, mean MAE loss: 6.095700 at the end of epoch by applying all training data 347\n",
      "For validation, mean MAE loss: 6.808576 at the end of epoch 347\n",
      "At 348-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.165468 during each epoch using batch data 348\n",
      "For training, mean MAE loss: 6.181132 at the end of epoch by applying all training data 348\n",
      "For validation, mean MAE loss: 6.774050 at the end of epoch 348\n",
      "At 349-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.161744 during each epoch using batch data 349\n",
      "For training, mean MAE loss: 6.115685 at the end of epoch by applying all training data 349\n",
      "For validation, mean MAE loss: 6.762625 at the end of epoch 349\n",
      "At 350-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.409204 during each epoch using batch data 350\n",
      "For training, mean MAE loss: 6.017298 at the end of epoch by applying all training data 350\n",
      "For validation, mean MAE loss: 6.771214 at the end of epoch 350\n",
      "At 351-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.214991 during each epoch using batch data 351\n",
      "For training, mean MAE loss: 6.062847 at the end of epoch by applying all training data 351\n",
      "For validation, mean MAE loss: 6.775827 at the end of epoch 351\n",
      "At 352-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.074808 during each epoch using batch data 352\n",
      "For training, mean MAE loss: 6.190829 at the end of epoch by applying all training data 352\n",
      "For validation, mean MAE loss: 6.796501 at the end of epoch 352\n",
      "At 353-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.105331 during each epoch using batch data 353\n",
      "For training, mean MAE loss: 5.975981 at the end of epoch by applying all training data 353\n",
      "For validation, mean MAE loss: 6.777957 at the end of epoch 353\n",
      "At 354-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.150679 during each epoch using batch data 354\n",
      "For training, mean MAE loss: 6.061865 at the end of epoch by applying all training data 354\n",
      "For validation, mean MAE loss: 6.747970 at the end of epoch 354\n",
      "At 355-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.998647 during each epoch using batch data 355\n",
      "For training, mean MAE loss: 6.219942 at the end of epoch by applying all training data 355\n",
      "For validation, mean MAE loss: 6.742921 at the end of epoch 355\n",
      "At 356-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.178979 during each epoch using batch data 356\n",
      "For training, mean MAE loss: 6.139915 at the end of epoch by applying all training data 356\n",
      "For validation, mean MAE loss: 6.771878 at the end of epoch 356\n",
      "At 357-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.161309 during each epoch using batch data 357\n",
      "For training, mean MAE loss: 6.113216 at the end of epoch by applying all training data 357\n",
      "For validation, mean MAE loss: 6.775386 at the end of epoch 357\n",
      "At 358-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.873867 during each epoch using batch data 358\n",
      "For training, mean MAE loss: 6.018885 at the end of epoch by applying all training data 358\n",
      "For validation, mean MAE loss: 6.758548 at the end of epoch 358\n",
      "At 359-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.999513 during each epoch using batch data 359\n",
      "For training, mean MAE loss: 6.065594 at the end of epoch by applying all training data 359\n",
      "For validation, mean MAE loss: 6.738419 at the end of epoch 359\n",
      "At 360-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.150567 during each epoch using batch data 360\n",
      "For training, mean MAE loss: 6.089997 at the end of epoch by applying all training data 360\n",
      "For validation, mean MAE loss: 6.767297 at the end of epoch 360\n",
      "At 361-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.163564 during each epoch using batch data 361\n",
      "For training, mean MAE loss: 6.276932 at the end of epoch by applying all training data 361\n",
      "For validation, mean MAE loss: 6.776951 at the end of epoch 361\n",
      "At 362-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.235304 during each epoch using batch data 362\n",
      "For training, mean MAE loss: 6.083661 at the end of epoch by applying all training data 362\n",
      "For validation, mean MAE loss: 6.751203 at the end of epoch 362\n",
      "At 363-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.095099 during each epoch using batch data 363\n",
      "For training, mean MAE loss: 6.213076 at the end of epoch by applying all training data 363\n",
      "For validation, mean MAE loss: 6.735246 at the end of epoch 363\n",
      "At 364-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.998120 during each epoch using batch data 364\n",
      "For training, mean MAE loss: 6.219983 at the end of epoch by applying all training data 364\n",
      "For validation, mean MAE loss: 6.736195 at the end of epoch 364\n",
      "At 365-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.244971 during each epoch using batch data 365\n",
      "For training, mean MAE loss: 6.088687 at the end of epoch by applying all training data 365\n",
      "For validation, mean MAE loss: 6.745084 at the end of epoch 365\n",
      "At 366-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.181898 during each epoch using batch data 366\n",
      "For training, mean MAE loss: 6.100295 at the end of epoch by applying all training data 366\n",
      "For validation, mean MAE loss: 6.735182 at the end of epoch 366\n",
      "At 367-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.146497 during each epoch using batch data 367\n",
      "For training, mean MAE loss: 6.135230 at the end of epoch by applying all training data 367\n",
      "For validation, mean MAE loss: 6.734790 at the end of epoch 367\n",
      "At 368-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.006398 during each epoch using batch data 368\n",
      "For training, mean MAE loss: 5.989435 at the end of epoch by applying all training data 368\n",
      "For validation, mean MAE loss: 6.724993 at the end of epoch 368\n",
      "At 369-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.959507 during each epoch using batch data 369\n",
      "For training, mean MAE loss: 6.097106 at the end of epoch by applying all training data 369\n",
      "For validation, mean MAE loss: 6.710211 at the end of epoch 369\n",
      "At 370-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.096738 during each epoch using batch data 370\n",
      "For training, mean MAE loss: 6.084390 at the end of epoch by applying all training data 370\n",
      "For validation, mean MAE loss: 6.717582 at the end of epoch 370\n",
      "At 371-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.008068 during each epoch using batch data 371\n",
      "For training, mean MAE loss: 5.921808 at the end of epoch by applying all training data 371\n",
      "For validation, mean MAE loss: 6.738771 at the end of epoch 371\n",
      "At 372-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.138194 during each epoch using batch data 372\n",
      "For training, mean MAE loss: 6.228186 at the end of epoch by applying all training data 372\n",
      "For validation, mean MAE loss: 6.733486 at the end of epoch 372\n",
      "At 373-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.011472 during each epoch using batch data 373\n",
      "For training, mean MAE loss: 6.091244 at the end of epoch by applying all training data 373\n",
      "For validation, mean MAE loss: 6.730979 at the end of epoch 373\n",
      "At 374-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.980898 during each epoch using batch data 374\n",
      "For training, mean MAE loss: 6.048014 at the end of epoch by applying all training data 374\n",
      "For validation, mean MAE loss: 6.725330 at the end of epoch 374\n",
      "At 375-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.913891 during each epoch using batch data 375\n",
      "For training, mean MAE loss: 6.121622 at the end of epoch by applying all training data 375\n",
      "For validation, mean MAE loss: 6.718181 at the end of epoch 375\n",
      "At 376-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.191665 during each epoch using batch data 376\n",
      "For training, mean MAE loss: 6.065321 at the end of epoch by applying all training data 376\n",
      "For validation, mean MAE loss: 6.721886 at the end of epoch 376\n",
      "At 377-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.124895 during each epoch using batch data 377\n",
      "For training, mean MAE loss: 6.093043 at the end of epoch by applying all training data 377\n",
      "For validation, mean MAE loss: 6.726503 at the end of epoch 377\n",
      "At 378-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.866910 during each epoch using batch data 378\n",
      "For training, mean MAE loss: 6.004199 at the end of epoch by applying all training data 378\n",
      "For validation, mean MAE loss: 6.750364 at the end of epoch 378\n",
      "At 379-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.129778 during each epoch using batch data 379\n",
      "For training, mean MAE loss: 6.098202 at the end of epoch by applying all training data 379\n",
      "For validation, mean MAE loss: 6.718081 at the end of epoch 379\n",
      "At 380-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.954156 during each epoch using batch data 380\n",
      "For training, mean MAE loss: 5.957815 at the end of epoch by applying all training data 380\n",
      "For validation, mean MAE loss: 6.734416 at the end of epoch 380\n",
      "At 381-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.053676 during each epoch using batch data 381\n",
      "For training, mean MAE loss: 6.121960 at the end of epoch by applying all training data 381\n",
      "For validation, mean MAE loss: 6.730257 at the end of epoch 381\n",
      "At 382-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.137887 during each epoch using batch data 382\n",
      "For training, mean MAE loss: 6.088826 at the end of epoch by applying all training data 382\n",
      "For validation, mean MAE loss: 6.711497 at the end of epoch 382\n",
      "At 383-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.180555 during each epoch using batch data 383\n",
      "For training, mean MAE loss: 6.100534 at the end of epoch by applying all training data 383\n",
      "For validation, mean MAE loss: 6.712139 at the end of epoch 383\n",
      "At 384-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.062286 during each epoch using batch data 384\n",
      "For training, mean MAE loss: 6.003548 at the end of epoch by applying all training data 384\n",
      "For validation, mean MAE loss: 6.693556 at the end of epoch 384\n",
      "At 385-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.259475 during each epoch using batch data 385\n",
      "For training, mean MAE loss: 6.303838 at the end of epoch by applying all training data 385\n",
      "For validation, mean MAE loss: 6.703295 at the end of epoch 385\n",
      "At 386-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.048340 during each epoch using batch data 386\n",
      "For training, mean MAE loss: 6.147412 at the end of epoch by applying all training data 386\n",
      "For validation, mean MAE loss: 6.702017 at the end of epoch 386\n",
      "At 387-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.025078 during each epoch using batch data 387\n",
      "For training, mean MAE loss: 6.174297 at the end of epoch by applying all training data 387\n",
      "For validation, mean MAE loss: 6.728816 at the end of epoch 387\n",
      "At 388-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.169080 during each epoch using batch data 388\n",
      "For training, mean MAE loss: 6.006126 at the end of epoch by applying all training data 388\n",
      "For validation, mean MAE loss: 6.715387 at the end of epoch 388\n",
      "At 389-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.138852 during each epoch using batch data 389\n",
      "For training, mean MAE loss: 6.066500 at the end of epoch by applying all training data 389\n",
      "For validation, mean MAE loss: 6.693725 at the end of epoch 389\n",
      "At 390-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.903691 during each epoch using batch data 390\n",
      "For training, mean MAE loss: 5.926484 at the end of epoch by applying all training data 390\n",
      "For validation, mean MAE loss: 6.723519 at the end of epoch 390\n",
      "At 391-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.995167 during each epoch using batch data 391\n",
      "For training, mean MAE loss: 6.187967 at the end of epoch by applying all training data 391\n",
      "For validation, mean MAE loss: 6.692129 at the end of epoch 391\n",
      "At 392-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.332979 during each epoch using batch data 392\n",
      "For training, mean MAE loss: 6.113973 at the end of epoch by applying all training data 392\n",
      "For validation, mean MAE loss: 6.680151 at the end of epoch 392\n",
      "At 393-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.127256 during each epoch using batch data 393\n",
      "For training, mean MAE loss: 6.258471 at the end of epoch by applying all training data 393\n",
      "For validation, mean MAE loss: 6.680800 at the end of epoch 393\n",
      "At 394-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.943097 during each epoch using batch data 394\n",
      "For training, mean MAE loss: 6.426317 at the end of epoch by applying all training data 394\n",
      "For validation, mean MAE loss: 6.706052 at the end of epoch 394\n",
      "At 395-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.004733 during each epoch using batch data 395\n",
      "For training, mean MAE loss: 5.996007 at the end of epoch by applying all training data 395\n",
      "For validation, mean MAE loss: 6.699215 at the end of epoch 395\n",
      "At 396-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.042642 during each epoch using batch data 396\n",
      "For training, mean MAE loss: 6.181171 at the end of epoch by applying all training data 396\n",
      "For validation, mean MAE loss: 6.706732 at the end of epoch 396\n",
      "At 397-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.010554 during each epoch using batch data 397\n",
      "For training, mean MAE loss: 5.947364 at the end of epoch by applying all training data 397\n",
      "For validation, mean MAE loss: 6.700467 at the end of epoch 397\n",
      "At 398-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.073383 during each epoch using batch data 398\n",
      "For training, mean MAE loss: 6.243837 at the end of epoch by applying all training data 398\n",
      "For validation, mean MAE loss: 6.691405 at the end of epoch 398\n",
      "At 399-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.106068 during each epoch using batch data 399\n",
      "For training, mean MAE loss: 6.194915 at the end of epoch by applying all training data 399\n",
      "For validation, mean MAE loss: 6.707218 at the end of epoch 399\n",
      "At 400-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.084562 during each epoch using batch data 400\n",
      "For training, mean MAE loss: 6.041565 at the end of epoch by applying all training data 400\n",
      "For validation, mean MAE loss: 6.690873 at the end of epoch 400\n",
      "At 401-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.904302 during each epoch using batch data 401\n",
      "For training, mean MAE loss: 6.066875 at the end of epoch by applying all training data 401\n",
      "For validation, mean MAE loss: 6.685444 at the end of epoch 401\n",
      "At 402-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.887837 during each epoch using batch data 402\n",
      "For training, mean MAE loss: 6.143953 at the end of epoch by applying all training data 402\n",
      "For validation, mean MAE loss: 6.693132 at the end of epoch 402\n",
      "At 403-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.926993 during each epoch using batch data 403\n",
      "For training, mean MAE loss: 5.853610 at the end of epoch by applying all training data 403\n",
      "For validation, mean MAE loss: 6.689657 at the end of epoch 403\n",
      "At 404-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.304761 during each epoch using batch data 404\n",
      "For training, mean MAE loss: 5.835595 at the end of epoch by applying all training data 404\n",
      "For validation, mean MAE loss: 6.681282 at the end of epoch 404\n",
      "At 405-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.149136 during each epoch using batch data 405\n",
      "For training, mean MAE loss: 5.873156 at the end of epoch by applying all training data 405\n",
      "For validation, mean MAE loss: 6.658360 at the end of epoch 405\n",
      "At 406-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.056916 during each epoch using batch data 406\n",
      "For training, mean MAE loss: 6.086801 at the end of epoch by applying all training data 406\n",
      "For validation, mean MAE loss: 6.681661 at the end of epoch 406\n",
      "At 407-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.879106 during each epoch using batch data 407\n",
      "For training, mean MAE loss: 6.302915 at the end of epoch by applying all training data 407\n",
      "For validation, mean MAE loss: 6.686855 at the end of epoch 407\n",
      "At 408-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.992652 during each epoch using batch data 408\n",
      "For training, mean MAE loss: 5.893959 at the end of epoch by applying all training data 408\n",
      "For validation, mean MAE loss: 6.665427 at the end of epoch 408\n",
      "At 409-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.143303 during each epoch using batch data 409\n",
      "For training, mean MAE loss: 5.946108 at the end of epoch by applying all training data 409\n",
      "For validation, mean MAE loss: 6.655128 at the end of epoch 409\n",
      "At 410-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.953102 during each epoch using batch data 410\n",
      "For training, mean MAE loss: 6.010582 at the end of epoch by applying all training data 410\n",
      "For validation, mean MAE loss: 6.689069 at the end of epoch 410\n",
      "At 411-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.011878 during each epoch using batch data 411\n",
      "For training, mean MAE loss: 5.832872 at the end of epoch by applying all training data 411\n",
      "For validation, mean MAE loss: 6.680440 at the end of epoch 411\n",
      "At 412-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.845992 during each epoch using batch data 412\n",
      "For training, mean MAE loss: 6.031572 at the end of epoch by applying all training data 412\n",
      "For validation, mean MAE loss: 6.652435 at the end of epoch 412\n",
      "At 413-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.890952 during each epoch using batch data 413\n",
      "For training, mean MAE loss: 6.090439 at the end of epoch by applying all training data 413\n",
      "For validation, mean MAE loss: 6.685500 at the end of epoch 413\n",
      "At 414-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.995224 during each epoch using batch data 414\n",
      "For training, mean MAE loss: 5.927406 at the end of epoch by applying all training data 414\n",
      "For validation, mean MAE loss: 6.716496 at the end of epoch 414\n",
      "At 415-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.051529 during each epoch using batch data 415\n",
      "For training, mean MAE loss: 6.067883 at the end of epoch by applying all training data 415\n",
      "For validation, mean MAE loss: 6.674475 at the end of epoch 415\n",
      "At 416-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.994629 during each epoch using batch data 416\n",
      "For training, mean MAE loss: 6.057194 at the end of epoch by applying all training data 416\n",
      "For validation, mean MAE loss: 6.666821 at the end of epoch 416\n",
      "At 417-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.246683 during each epoch using batch data 417\n",
      "For training, mean MAE loss: 6.301950 at the end of epoch by applying all training data 417\n",
      "For validation, mean MAE loss: 6.658315 at the end of epoch 417\n",
      "At 418-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.931768 during each epoch using batch data 418\n",
      "For training, mean MAE loss: 5.958994 at the end of epoch by applying all training data 418\n",
      "For validation, mean MAE loss: 6.690691 at the end of epoch 418\n",
      "At 419-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 6.004082 during each epoch using batch data 419\n",
      "For training, mean MAE loss: 6.106402 at the end of epoch by applying all training data 419\n",
      "For validation, mean MAE loss: 6.700505 at the end of epoch 419\n",
      "At 420-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.984548 during each epoch using batch data 420\n",
      "For training, mean MAE loss: 6.126423 at the end of epoch by applying all training data 420\n",
      "For validation, mean MAE loss: 6.695257 at the end of epoch 420\n",
      "At 421-th epoch.\n",
      "The number of batches in this sampler based on the batch size: 10\n",
      "For training, mean MAE loss: 5.932372 during each epoch using batch data 421\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreg_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m regression for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morgan_system\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Run regression\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     reg_function(feature_tsv_path, output_dir, cv_repetition)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreg_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m regression completed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morgan_system\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Results saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Research/mlni/mlni/adml_regression_nn.py:52\u001b[0m, in \u001b[0;36mregression_roi\u001b[0;34m(feature_tsv, output_dir, cv_repetition, cv_strategy, batch_size, epochs, lr, weight_decay, n_threads, seed, verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m # ## CHANGED\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m # from mlni.regression_nn import RB_RepeatedHoldOut_NN_Regression\n\u001b[1;32m      5\u001b[0m # from mlni.base import RB_Input\n\u001b[1;32m      6\u001b[0m # import os, pickle\n\u001b[1;32m      7\u001b[0m # from mlni.utils import make_cv_partition\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m # def regression_roi(feature_tsv, output_dir, cv_repetition, cv_strategy='hold_out', \n\u001b[1;32m     10\u001b[0m #                   batch_size=64, epochs=1500, lr=0.0001, weight_decay=1e-4, \n\u001b[1;32m     11\u001b[0m #                   n_threads=8, seed=None, verbose=False):\n\u001b[1;32m     12\u001b[0m #     \"\"\"\n\u001b[1;32m     13\u001b[0m #     Core function for regression with ROI-based features using Neural Network regression\n\u001b[1;32m     14\u001b[0m #     Args:\n\u001b[1;32m     15\u001b[0m #         feature_tsv: str, path to the tsv containing extracted feature\n\u001b[1;32m     16\u001b[0m #         output_dir: str, path to store the regression results\n\u001b[1;32m     17\u001b[0m #         cv_repetition: int, number of repetitions for cross-validation (CV)\n\u001b[1;32m     18\u001b[0m #         cv_strategy: str, cross validation strategy used. Default is hold_out\n\u001b[1;32m     19\u001b[0m #         batch_size: int, size of batches for training\n\u001b[1;32m     20\u001b[0m #         epochs: int, number of training epochs\n\u001b[1;32m     21\u001b[0m #         lr: float, learning rate\n\u001b[1;32m     22\u001b[0m #         weight_decay: float, weight decay for optimization\n\u001b[1;32m     23\u001b[0m #         n_threads: int, number of threads to use\n\u001b[1;32m     24\u001b[0m #         seed: int, random seed for reproducibility\n\u001b[1;32m     25\u001b[0m #         verbose: bool, whether to print verbose output\n\u001b[1;32m     26\u001b[0m #     Returns: regression outputs\n\u001b[1;32m     27\u001b[0m #     \"\"\"\n\u001b[1;32m     28\u001b[0m #     print('MLNI for a regression with nested CV...')\n\u001b[1;32m     29\u001b[0m #     input_data = RB_Input(feature_tsv, standardization_method=\"minmax\")\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m #     print('Data split was performed based on validation strategy: %s...\\n' % cv_strategy)\n\u001b[1;32m     32\u001b[0m #     if os.path.isfile(os.path.join(output_dir, 'data_split_stratified_' + str(cv_repetition) + '-holdout.pkl')):\n\u001b[1;32m     33\u001b[0m #         split_index = pickle.load(open(os.path.join(output_dir, 'data_split_stratified_' + str(cv_repetition) + '-holdout.pkl'), 'rb'))\n\u001b[1;32m     34\u001b[0m #     else:\n\u001b[1;32m     35\u001b[0m #         split_index, _ = make_cv_partition(input_data.get_y(), cv_strategy, output_dir, cv_repetition, seed=seed)\n\u001b[1;32m     36\u001b[0m #     print('Data split has been done!\\n')\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m #     print('Starts regression with NN...')\n\u001b[1;32m     39\u001b[0m #     if cv_strategy == 'hold_out':\n\u001b[1;32m     40\u001b[0m #         wf_regression = RB_RepeatedHoldOut_NN_Regression(\n\u001b[1;32m     41\u001b[0m #             input_data, \n\u001b[1;32m     42\u001b[0m #             split_index, \n\u001b[1;32m     43\u001b[0m #             os.path.join(output_dir, 'regression'),\n\u001b[1;32m     44\u001b[0m #             n_threads=n_threads, \n\u001b[1;32m     45\u001b[0m #             n_iterations=cv_repetition, \n\u001b[1;32m     46\u001b[0m #             batch_size=batch_size,\n\u001b[1;32m     47\u001b[0m #             epochs=epochs, \n\u001b[1;32m     48\u001b[0m #             lr=lr, \n\u001b[1;32m     49\u001b[0m #             weight_decay=weight_decay,\n\u001b[1;32m     50\u001b[0m #             verbose=verbose\n\u001b[1;32m     51\u001b[0m #         )\n\u001b[0;32m---> 52\u001b[0m #         wf_regression.run()\n\u001b[1;32m     53\u001b[0m #     else:\n\u001b[1;32m     54\u001b[0m #         raise Exception(\"CV methods have not been implemented\")\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m #     print('Finish...')\n\u001b[1;32m     59\u001b[0m from mlni.regression_nn import RB_RepeatedHoldOut_NN_Regression\n\u001b[1;32m     60\u001b[0m from mlni.base import RB_Input\n",
      "File \u001b[0;32m~/Documents/Research/mlni/mlni/regression_nn.py:56\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      1\u001b[0m # # CHANGED\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m # from mlni.base import WorkFlow, RegressionAlgorithm, RegressionValidation\n\u001b[1;32m      4\u001b[0m # import numpy as np\n\u001b[1;32m      5\u001b[0m # from sklearn.model_selection import ShuffleSplit\n\u001b[1;32m      6\u001b[0m # from mlni.utils import time_bar, neural_network_regression_3LinerLayers, neural_network_regression_5LinerLayers, train_network\n\u001b[1;32m      7\u001b[0m # import torch\n\u001b[1;32m      8\u001b[0m # import copy\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m # class RB_RepeatedHoldOut_NN_Regression(WorkFlow):\n\u001b[1;32m     11\u001b[0m #     \"\"\"\n\u001b[1;32m     12\u001b[0m #     The main class to run MLNI with repeated holdout CV for regression.\n\u001b[1;32m     13\u001b[0m #     \"\"\"\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m #     def __init__(self, input, split_index, output_dir, n_threads=8, n_iterations=100, test_size=0.2,\n\u001b[1;32m     16\u001b[0m #                  batch_size=64, epochs=10, lr=0.0001, weight_decay=1e-4, verbose=False):\n\u001b[1;32m     17\u001b[0m #         self._input = input\n\u001b[1;32m     18\u001b[0m #         self._split_index = split_index\n\u001b[1;32m     19\u001b[0m #         self._output_dir = output_dir\n\u001b[1;32m     20\u001b[0m #         self._n_threads = n_threads\n\u001b[1;32m     21\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m     22\u001b[0m #         self._batch_size = batch_size\n\u001b[1;32m     23\u001b[0m #         self._epochs = epochs\n\u001b[1;32m     24\u001b[0m #         self._lr = lr\n\u001b[1;32m     25\u001b[0m #         self._weight_decay = weight_decay\n\u001b[1;32m     26\u001b[0m #         self._verbose = verbose\n\u001b[1;32m     27\u001b[0m #         self._test_size = test_size\n\u001b[1;32m     28\u001b[0m #         self._validation = None\n\u001b[1;32m     29\u001b[0m #         self._algorithm = None\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m #     def run(self):\n\u001b[1;32m     32\u001b[0m #         x = self._input.get_x()\n\u001b[1;32m     33\u001b[0m #         y = self._input.get_y_raw()\n\u001b[1;32m     34\u001b[0m #         df_header = self._input.get_participant_session_id()\n\u001b[1;32m     35\u001b[0m #         input_dim = x.shape[1]\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m #         x_tensor = torch.FloatTensor(x)\n\u001b[1;32m     38\u001b[0m #         y_tensor = torch.FloatTensor(y)\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m #         self._algorithm = NNRegressionAlgorithm(\n\u001b[1;32m     41\u001b[0m #             x_tensor, y_tensor, self._output_dir, df_header, \n\u001b[1;32m     42\u001b[0m #             self._n_iterations,\n\u001b[1;32m     43\u001b[0m #             batch_size=self._batch_size,\n\u001b[1;32m     44\u001b[0m #             epochs=self._epochs,\n\u001b[1;32m     45\u001b[0m #             lr=self._lr,\n\u001b[1;32m     46\u001b[0m #             weight_decay=self._weight_decay, \n\u001b[1;32m     47\u001b[0m #             verbose=self._verbose\n\u001b[1;32m     48\u001b[0m #         )\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m #         self._validation = RepeatedHoldOut(\n\u001b[1;32m     51\u001b[0m #             self._algorithm, input_dim, \n\u001b[1;32m     52\u001b[0m #             n_iterations=self._n_iterations, \n\u001b[1;32m     53\u001b[0m #             test_size=self._test_size\n\u001b[1;32m     54\u001b[0m #         )\n\u001b[1;32m     55\u001b[0m \n\u001b[0;32m---> 56\u001b[0m #         self._validation.validate(y, splits_indices=self._split_index, verbose=self._verbose)\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m # class NNRegressionAlgorithm(RegressionAlgorithm):\n\u001b[1;32m     59\u001b[0m #     '''\n\u001b[1;32m     60\u001b[0m #     NN regression.\n\u001b[1;32m     61\u001b[0m #     '''\n\u001b[1;32m     62\u001b[0m #     def __init__(self, x, y, output_dir, df_header, n_iterations=100, batch_size=64, \n\u001b[1;32m     63\u001b[0m #                  epochs=10, lr=0.0001, weight_decay=1e-4, verbose=False):\n\u001b[1;32m     64\u001b[0m #         self._x = x\n\u001b[1;32m     65\u001b[0m #         self._y = y\n\u001b[1;32m     66\u001b[0m #         self._output_dir = output_dir\n\u001b[1;32m     67\u001b[0m #         self._df_header = df_header\n\u001b[1;32m     68\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m     69\u001b[0m #         self._batch_size = batch_size\n\u001b[1;32m     70\u001b[0m #         self._epochs = epochs\n\u001b[1;32m     71\u001b[0m #         self._lr = lr\n\u001b[1;32m     72\u001b[0m #         self._weight_decay = weight_decay\n\u001b[1;32m     73\u001b[0m #         self._verbose = verbose\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m #     def _lauch_nn(self, input_dim, x_train, x_test, y_train, y_test, fi):\n\u001b[1;32m     76\u001b[0m #         model = neural_network_regression_3LinerLayers(input_dim)\n\u001b[1;32m     77\u001b[0m #         model.cpu()\n\u001b[1;32m     78\u001b[0m #         init_state = copy.deepcopy(model.state_dict())\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m #         # train the NN - passing optimizer as string name\n\u001b[1;32m     81\u001b[0m #         train_network(\n\u001b[1;32m     82\u001b[0m #             model, \n\u001b[1;32m     83\u001b[0m #             self._output_dir, \n\u001b[1;32m     84\u001b[0m #             fi, \n\u001b[1;32m     85\u001b[0m #             x_train, \n\u001b[1;32m     86\u001b[0m #             y_train, \n\u001b[1;32m     87\u001b[0m #             x_test, \n\u001b[1;32m     88\u001b[0m #             y_test, \n\u001b[1;32m     89\u001b[0m #             self._epochs,\n\u001b[1;32m     90\u001b[0m #             self._batch_size, \n\u001b[1;32m     91\u001b[0m #             init_state, \n\u001b[1;32m     92\u001b[0m #             self._df_header,\n\u001b[1;32m     93\u001b[0m #             gpu=False,\n\u001b[1;32m     94\u001b[0m #             lr=self._lr, \n\u001b[1;32m     95\u001b[0m #             weight_decay=self._weight_decay,\n\u001b[1;32m     96\u001b[0m #             opt='Adam'  # Pass optimizer name as string\n\u001b[1;32m     97\u001b[0m #         )\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m #     def evaluate(self, input_dim, train_index, test_index, fi):\n\u001b[1;32m    100\u001b[0m #         x_train = self._x[train_index]\n\u001b[1;32m    101\u001b[0m #         y_train = self._y[train_index]\n\u001b[1;32m    102\u001b[0m #         x_test = self._x[test_index]\n\u001b[1;32m    103\u001b[0m #         y_test = self._y[test_index]\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m #         results = self._lauch_nn(input_dim, x_train, x_test, y_train, y_test, fi)\n\u001b[1;32m    106\u001b[0m #         return results\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m # class RepeatedHoldOut(RegressionValidation):\n\u001b[1;32m    109\u001b[0m #     \"\"\"\n\u001b[1;32m    110\u001b[0m #     Repeated holdout splits CV.\n\u001b[1;32m    111\u001b[0m #     \"\"\"\n\u001b[1;32m    112\u001b[0m #     def __init__(self, ml_algorithm, input_dim, n_iterations=100, test_size=0.3):\n\u001b[1;32m    113\u001b[0m #         self._ml_algorithm = ml_algorithm\n\u001b[1;32m    114\u001b[0m #         self._input_dim = input_dim\n\u001b[1;32m    115\u001b[0m #         self._split_results = []\n\u001b[1;32m    116\u001b[0m #         self._cv = None\n\u001b[1;32m    117\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m    118\u001b[0m #         self._test_size = test_size\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m #     def validate(self, y, splits_indices=None, verbose=False):\n\u001b[1;32m    121\u001b[0m #         if splits_indices is None:\n\u001b[1;32m    122\u001b[0m #             splits = ShuffleSplit(n_splits=self._n_iterations, test_size=self._test_size)\n\u001b[1;32m    123\u001b[0m #             self._cv = list(splits.split(np.zeros(len(y)), y))\n\u001b[1;32m    124\u001b[0m #         else:\n\u001b[1;32m    125\u001b[0m #             self._cv = splits_indices\n\u001b[1;32m    126\u001b[0m #         results = {}\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m #         for i in range(self._n_iterations):\n\u001b[1;32m    129\u001b[0m #             time_bar(i, self._n_iterations)\n\u001b[1;32m    130\u001b[0m #             print()\n\u001b[1;32m    131\u001b[0m #             if verbose:\n\u001b[1;32m    132\u001b[0m #                 print(\"Repetition %d of CV...\" % i)\n\u001b[1;32m    133\u001b[0m #             train_index, test_index = self._cv[i]\n\u001b[1;32m    134\u001b[0m #             results[i] = self._ml_algorithm.evaluate(self._input_dim, train_index, test_index, i)\n\u001b[1;32m    136\u001b[0m from mlni.base import WorkFlow, RegressionAlgorithm, RegressionValidation\n\u001b[1;32m    137\u001b[0m import numpy as np\n",
      "File \u001b[0;32m~/Documents/Research/mlni/mlni/regression_nn.py:134\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(self, y, splits_indices, verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m # # CHANGED\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m # from mlni.base import WorkFlow, RegressionAlgorithm, RegressionValidation\n\u001b[1;32m      4\u001b[0m # import numpy as np\n\u001b[1;32m      5\u001b[0m # from sklearn.model_selection import ShuffleSplit\n\u001b[1;32m      6\u001b[0m # from mlni.utils import time_bar, neural_network_regression_3LinerLayers, neural_network_regression_5LinerLayers, train_network\n\u001b[1;32m      7\u001b[0m # import torch\n\u001b[1;32m      8\u001b[0m # import copy\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m # class RB_RepeatedHoldOut_NN_Regression(WorkFlow):\n\u001b[1;32m     11\u001b[0m #     \"\"\"\n\u001b[1;32m     12\u001b[0m #     The main class to run MLNI with repeated holdout CV for regression.\n\u001b[1;32m     13\u001b[0m #     \"\"\"\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m #     def __init__(self, input, split_index, output_dir, n_threads=8, n_iterations=100, test_size=0.2,\n\u001b[1;32m     16\u001b[0m #                  batch_size=64, epochs=10, lr=0.0001, weight_decay=1e-4, verbose=False):\n\u001b[1;32m     17\u001b[0m #         self._input = input\n\u001b[1;32m     18\u001b[0m #         self._split_index = split_index\n\u001b[1;32m     19\u001b[0m #         self._output_dir = output_dir\n\u001b[1;32m     20\u001b[0m #         self._n_threads = n_threads\n\u001b[1;32m     21\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m     22\u001b[0m #         self._batch_size = batch_size\n\u001b[1;32m     23\u001b[0m #         self._epochs = epochs\n\u001b[1;32m     24\u001b[0m #         self._lr = lr\n\u001b[1;32m     25\u001b[0m #         self._weight_decay = weight_decay\n\u001b[1;32m     26\u001b[0m #         self._verbose = verbose\n\u001b[1;32m     27\u001b[0m #         self._test_size = test_size\n\u001b[1;32m     28\u001b[0m #         self._validation = None\n\u001b[1;32m     29\u001b[0m #         self._algorithm = None\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m #     def run(self):\n\u001b[1;32m     32\u001b[0m #         x = self._input.get_x()\n\u001b[1;32m     33\u001b[0m #         y = self._input.get_y_raw()\n\u001b[1;32m     34\u001b[0m #         df_header = self._input.get_participant_session_id()\n\u001b[1;32m     35\u001b[0m #         input_dim = x.shape[1]\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m #         x_tensor = torch.FloatTensor(x)\n\u001b[1;32m     38\u001b[0m #         y_tensor = torch.FloatTensor(y)\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m #         self._algorithm = NNRegressionAlgorithm(\n\u001b[1;32m     41\u001b[0m #             x_tensor, y_tensor, self._output_dir, df_header, \n\u001b[1;32m     42\u001b[0m #             self._n_iterations,\n\u001b[1;32m     43\u001b[0m #             batch_size=self._batch_size,\n\u001b[1;32m     44\u001b[0m #             epochs=self._epochs,\n\u001b[1;32m     45\u001b[0m #             lr=self._lr,\n\u001b[1;32m     46\u001b[0m #             weight_decay=self._weight_decay, \n\u001b[1;32m     47\u001b[0m #             verbose=self._verbose\n\u001b[1;32m     48\u001b[0m #         )\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m #         self._validation = RepeatedHoldOut(\n\u001b[1;32m     51\u001b[0m #             self._algorithm, input_dim, \n\u001b[1;32m     52\u001b[0m #             n_iterations=self._n_iterations, \n\u001b[1;32m     53\u001b[0m #             test_size=self._test_size\n\u001b[1;32m     54\u001b[0m #         )\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m #         self._validation.validate(y, splits_indices=self._split_index, verbose=self._verbose)\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m # class NNRegressionAlgorithm(RegressionAlgorithm):\n\u001b[1;32m     59\u001b[0m #     '''\n\u001b[1;32m     60\u001b[0m #     NN regression.\n\u001b[1;32m     61\u001b[0m #     '''\n\u001b[1;32m     62\u001b[0m #     def __init__(self, x, y, output_dir, df_header, n_iterations=100, batch_size=64, \n\u001b[1;32m     63\u001b[0m #                  epochs=10, lr=0.0001, weight_decay=1e-4, verbose=False):\n\u001b[1;32m     64\u001b[0m #         self._x = x\n\u001b[1;32m     65\u001b[0m #         self._y = y\n\u001b[1;32m     66\u001b[0m #         self._output_dir = output_dir\n\u001b[1;32m     67\u001b[0m #         self._df_header = df_header\n\u001b[1;32m     68\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m     69\u001b[0m #         self._batch_size = batch_size\n\u001b[1;32m     70\u001b[0m #         self._epochs = epochs\n\u001b[1;32m     71\u001b[0m #         self._lr = lr\n\u001b[1;32m     72\u001b[0m #         self._weight_decay = weight_decay\n\u001b[1;32m     73\u001b[0m #         self._verbose = verbose\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m #     def _lauch_nn(self, input_dim, x_train, x_test, y_train, y_test, fi):\n\u001b[1;32m     76\u001b[0m #         model = neural_network_regression_3LinerLayers(input_dim)\n\u001b[1;32m     77\u001b[0m #         model.cpu()\n\u001b[1;32m     78\u001b[0m #         init_state = copy.deepcopy(model.state_dict())\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m #         # train the NN - passing optimizer as string name\n\u001b[1;32m     81\u001b[0m #         train_network(\n\u001b[1;32m     82\u001b[0m #             model, \n\u001b[1;32m     83\u001b[0m #             self._output_dir, \n\u001b[1;32m     84\u001b[0m #             fi, \n\u001b[1;32m     85\u001b[0m #             x_train, \n\u001b[1;32m     86\u001b[0m #             y_train, \n\u001b[1;32m     87\u001b[0m #             x_test, \n\u001b[1;32m     88\u001b[0m #             y_test, \n\u001b[1;32m     89\u001b[0m #             self._epochs,\n\u001b[1;32m     90\u001b[0m #             self._batch_size, \n\u001b[1;32m     91\u001b[0m #             init_state, \n\u001b[1;32m     92\u001b[0m #             self._df_header,\n\u001b[1;32m     93\u001b[0m #             gpu=False,\n\u001b[1;32m     94\u001b[0m #             lr=self._lr, \n\u001b[1;32m     95\u001b[0m #             weight_decay=self._weight_decay,\n\u001b[1;32m     96\u001b[0m #             opt='Adam'  # Pass optimizer name as string\n\u001b[1;32m     97\u001b[0m #         )\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m #     def evaluate(self, input_dim, train_index, test_index, fi):\n\u001b[1;32m    100\u001b[0m #         x_train = self._x[train_index]\n\u001b[1;32m    101\u001b[0m #         y_train = self._y[train_index]\n\u001b[1;32m    102\u001b[0m #         x_test = self._x[test_index]\n\u001b[1;32m    103\u001b[0m #         y_test = self._y[test_index]\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m #         results = self._lauch_nn(input_dim, x_train, x_test, y_train, y_test, fi)\n\u001b[1;32m    106\u001b[0m #         return results\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m # class RepeatedHoldOut(RegressionValidation):\n\u001b[1;32m    109\u001b[0m #     \"\"\"\n\u001b[1;32m    110\u001b[0m #     Repeated holdout splits CV.\n\u001b[1;32m    111\u001b[0m #     \"\"\"\n\u001b[1;32m    112\u001b[0m #     def __init__(self, ml_algorithm, input_dim, n_iterations=100, test_size=0.3):\n\u001b[1;32m    113\u001b[0m #         self._ml_algorithm = ml_algorithm\n\u001b[1;32m    114\u001b[0m #         self._input_dim = input_dim\n\u001b[1;32m    115\u001b[0m #         self._split_results = []\n\u001b[1;32m    116\u001b[0m #         self._cv = None\n\u001b[1;32m    117\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m    118\u001b[0m #         self._test_size = test_size\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m #     def validate(self, y, splits_indices=None, verbose=False):\n\u001b[1;32m    121\u001b[0m #         if splits_indices is None:\n\u001b[1;32m    122\u001b[0m #             splits = ShuffleSplit(n_splits=self._n_iterations, test_size=self._test_size)\n\u001b[1;32m    123\u001b[0m #             self._cv = list(splits.split(np.zeros(len(y)), y))\n\u001b[1;32m    124\u001b[0m #         else:\n\u001b[1;32m    125\u001b[0m #             self._cv = splits_indices\n\u001b[1;32m    126\u001b[0m #         results = {}\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m #         for i in range(self._n_iterations):\n\u001b[1;32m    129\u001b[0m #             time_bar(i, self._n_iterations)\n\u001b[1;32m    130\u001b[0m #             print()\n\u001b[1;32m    131\u001b[0m #             if verbose:\n\u001b[1;32m    132\u001b[0m #                 print(\"Repetition %d of CV...\" % i)\n\u001b[1;32m    133\u001b[0m #             train_index, test_index = self._cv[i]\n\u001b[0;32m--> 134\u001b[0m #             results[i] = self._ml_algorithm.evaluate(self._input_dim, train_index, test_index, i)\n\u001b[1;32m    136\u001b[0m from mlni.base import WorkFlow, RegressionAlgorithm, RegressionValidation\n\u001b[1;32m    137\u001b[0m import numpy as np\n",
      "File \u001b[0;32m~/Documents/Research/mlni/mlni/regression_nn.py:105\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(self, input_dim, train_index, test_index, fi)\u001b[0m\n\u001b[1;32m      1\u001b[0m # # CHANGED\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m # from mlni.base import WorkFlow, RegressionAlgorithm, RegressionValidation\n\u001b[1;32m      4\u001b[0m # import numpy as np\n\u001b[1;32m      5\u001b[0m # from sklearn.model_selection import ShuffleSplit\n\u001b[1;32m      6\u001b[0m # from mlni.utils import time_bar, neural_network_regression_3LinerLayers, neural_network_regression_5LinerLayers, train_network\n\u001b[1;32m      7\u001b[0m # import torch\n\u001b[1;32m      8\u001b[0m # import copy\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m # class RB_RepeatedHoldOut_NN_Regression(WorkFlow):\n\u001b[1;32m     11\u001b[0m #     \"\"\"\n\u001b[1;32m     12\u001b[0m #     The main class to run MLNI with repeated holdout CV for regression.\n\u001b[1;32m     13\u001b[0m #     \"\"\"\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m #     def __init__(self, input, split_index, output_dir, n_threads=8, n_iterations=100, test_size=0.2,\n\u001b[1;32m     16\u001b[0m #                  batch_size=64, epochs=10, lr=0.0001, weight_decay=1e-4, verbose=False):\n\u001b[1;32m     17\u001b[0m #         self._input = input\n\u001b[1;32m     18\u001b[0m #         self._split_index = split_index\n\u001b[1;32m     19\u001b[0m #         self._output_dir = output_dir\n\u001b[1;32m     20\u001b[0m #         self._n_threads = n_threads\n\u001b[1;32m     21\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m     22\u001b[0m #         self._batch_size = batch_size\n\u001b[1;32m     23\u001b[0m #         self._epochs = epochs\n\u001b[1;32m     24\u001b[0m #         self._lr = lr\n\u001b[1;32m     25\u001b[0m #         self._weight_decay = weight_decay\n\u001b[1;32m     26\u001b[0m #         self._verbose = verbose\n\u001b[1;32m     27\u001b[0m #         self._test_size = test_size\n\u001b[1;32m     28\u001b[0m #         self._validation = None\n\u001b[1;32m     29\u001b[0m #         self._algorithm = None\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m #     def run(self):\n\u001b[1;32m     32\u001b[0m #         x = self._input.get_x()\n\u001b[1;32m     33\u001b[0m #         y = self._input.get_y_raw()\n\u001b[1;32m     34\u001b[0m #         df_header = self._input.get_participant_session_id()\n\u001b[1;32m     35\u001b[0m #         input_dim = x.shape[1]\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m #         x_tensor = torch.FloatTensor(x)\n\u001b[1;32m     38\u001b[0m #         y_tensor = torch.FloatTensor(y)\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m #         self._algorithm = NNRegressionAlgorithm(\n\u001b[1;32m     41\u001b[0m #             x_tensor, y_tensor, self._output_dir, df_header, \n\u001b[1;32m     42\u001b[0m #             self._n_iterations,\n\u001b[1;32m     43\u001b[0m #             batch_size=self._batch_size,\n\u001b[1;32m     44\u001b[0m #             epochs=self._epochs,\n\u001b[1;32m     45\u001b[0m #             lr=self._lr,\n\u001b[1;32m     46\u001b[0m #             weight_decay=self._weight_decay, \n\u001b[1;32m     47\u001b[0m #             verbose=self._verbose\n\u001b[1;32m     48\u001b[0m #         )\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m #         self._validation = RepeatedHoldOut(\n\u001b[1;32m     51\u001b[0m #             self._algorithm, input_dim, \n\u001b[1;32m     52\u001b[0m #             n_iterations=self._n_iterations, \n\u001b[1;32m     53\u001b[0m #             test_size=self._test_size\n\u001b[1;32m     54\u001b[0m #         )\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m #         self._validation.validate(y, splits_indices=self._split_index, verbose=self._verbose)\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m # class NNRegressionAlgorithm(RegressionAlgorithm):\n\u001b[1;32m     59\u001b[0m #     '''\n\u001b[1;32m     60\u001b[0m #     NN regression.\n\u001b[1;32m     61\u001b[0m #     '''\n\u001b[1;32m     62\u001b[0m #     def __init__(self, x, y, output_dir, df_header, n_iterations=100, batch_size=64, \n\u001b[1;32m     63\u001b[0m #                  epochs=10, lr=0.0001, weight_decay=1e-4, verbose=False):\n\u001b[1;32m     64\u001b[0m #         self._x = x\n\u001b[1;32m     65\u001b[0m #         self._y = y\n\u001b[1;32m     66\u001b[0m #         self._output_dir = output_dir\n\u001b[1;32m     67\u001b[0m #         self._df_header = df_header\n\u001b[1;32m     68\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m     69\u001b[0m #         self._batch_size = batch_size\n\u001b[1;32m     70\u001b[0m #         self._epochs = epochs\n\u001b[1;32m     71\u001b[0m #         self._lr = lr\n\u001b[1;32m     72\u001b[0m #         self._weight_decay = weight_decay\n\u001b[1;32m     73\u001b[0m #         self._verbose = verbose\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m #     def _lauch_nn(self, input_dim, x_train, x_test, y_train, y_test, fi):\n\u001b[1;32m     76\u001b[0m #         model = neural_network_regression_3LinerLayers(input_dim)\n\u001b[1;32m     77\u001b[0m #         model.cpu()\n\u001b[1;32m     78\u001b[0m #         init_state = copy.deepcopy(model.state_dict())\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m #         # train the NN - passing optimizer as string name\n\u001b[1;32m     81\u001b[0m #         train_network(\n\u001b[1;32m     82\u001b[0m #             model, \n\u001b[1;32m     83\u001b[0m #             self._output_dir, \n\u001b[1;32m     84\u001b[0m #             fi, \n\u001b[1;32m     85\u001b[0m #             x_train, \n\u001b[1;32m     86\u001b[0m #             y_train, \n\u001b[1;32m     87\u001b[0m #             x_test, \n\u001b[1;32m     88\u001b[0m #             y_test, \n\u001b[1;32m     89\u001b[0m #             self._epochs,\n\u001b[1;32m     90\u001b[0m #             self._batch_size, \n\u001b[1;32m     91\u001b[0m #             init_state, \n\u001b[1;32m     92\u001b[0m #             self._df_header,\n\u001b[1;32m     93\u001b[0m #             gpu=False,\n\u001b[1;32m     94\u001b[0m #             lr=self._lr, \n\u001b[1;32m     95\u001b[0m #             weight_decay=self._weight_decay,\n\u001b[1;32m     96\u001b[0m #             opt='Adam'  # Pass optimizer name as string\n\u001b[1;32m     97\u001b[0m #         )\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m #     def evaluate(self, input_dim, train_index, test_index, fi):\n\u001b[1;32m    100\u001b[0m #         x_train = self._x[train_index]\n\u001b[1;32m    101\u001b[0m #         y_train = self._y[train_index]\n\u001b[1;32m    102\u001b[0m #         x_test = self._x[test_index]\n\u001b[1;32m    103\u001b[0m #         y_test = self._y[test_index]\n\u001b[1;32m    104\u001b[0m \n\u001b[0;32m--> 105\u001b[0m #         results = self._lauch_nn(input_dim, x_train, x_test, y_train, y_test, fi)\n\u001b[1;32m    106\u001b[0m #         return results\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m # class RepeatedHoldOut(RegressionValidation):\n\u001b[1;32m    109\u001b[0m #     \"\"\"\n\u001b[1;32m    110\u001b[0m #     Repeated holdout splits CV.\n\u001b[1;32m    111\u001b[0m #     \"\"\"\n\u001b[1;32m    112\u001b[0m #     def __init__(self, ml_algorithm, input_dim, n_iterations=100, test_size=0.3):\n\u001b[1;32m    113\u001b[0m #         self._ml_algorithm = ml_algorithm\n\u001b[1;32m    114\u001b[0m #         self._input_dim = input_dim\n\u001b[1;32m    115\u001b[0m #         self._split_results = []\n\u001b[1;32m    116\u001b[0m #         self._cv = None\n\u001b[1;32m    117\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m    118\u001b[0m #         self._test_size = test_size\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m #     def validate(self, y, splits_indices=None, verbose=False):\n\u001b[1;32m    121\u001b[0m #         if splits_indices is None:\n\u001b[1;32m    122\u001b[0m #             splits = ShuffleSplit(n_splits=self._n_iterations, test_size=self._test_size)\n\u001b[1;32m    123\u001b[0m #             self._cv = list(splits.split(np.zeros(len(y)), y))\n\u001b[1;32m    124\u001b[0m #         else:\n\u001b[1;32m    125\u001b[0m #             self._cv = splits_indices\n\u001b[1;32m    126\u001b[0m #         results = {}\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m #         for i in range(self._n_iterations):\n\u001b[1;32m    129\u001b[0m #             time_bar(i, self._n_iterations)\n\u001b[1;32m    130\u001b[0m #             print()\n\u001b[1;32m    131\u001b[0m #             if verbose:\n\u001b[1;32m    132\u001b[0m #                 print(\"Repetition %d of CV...\" % i)\n\u001b[1;32m    133\u001b[0m #             train_index, test_index = self._cv[i]\n\u001b[1;32m    134\u001b[0m #             results[i] = self._ml_algorithm.evaluate(self._input_dim, train_index, test_index, i)\n\u001b[1;32m    136\u001b[0m from mlni.base import WorkFlow, RegressionAlgorithm, RegressionValidation\n\u001b[1;32m    137\u001b[0m import numpy as np\n",
      "File \u001b[0;32m~/Documents/Research/mlni/mlni/regression_nn.py:81\u001b[0m, in \u001b[0;36m_lauch_nn\u001b[0;34m(self, input_dim, x_train, x_test, y_train, y_test, fi)\u001b[0m\n\u001b[1;32m      1\u001b[0m # # CHANGED\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m # from mlni.base import WorkFlow, RegressionAlgorithm, RegressionValidation\n\u001b[1;32m      4\u001b[0m # import numpy as np\n\u001b[1;32m      5\u001b[0m # from sklearn.model_selection import ShuffleSplit\n\u001b[1;32m      6\u001b[0m # from mlni.utils import time_bar, neural_network_regression_3LinerLayers, neural_network_regression_5LinerLayers, train_network\n\u001b[1;32m      7\u001b[0m # import torch\n\u001b[1;32m      8\u001b[0m # import copy\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m # class RB_RepeatedHoldOut_NN_Regression(WorkFlow):\n\u001b[1;32m     11\u001b[0m #     \"\"\"\n\u001b[1;32m     12\u001b[0m #     The main class to run MLNI with repeated holdout CV for regression.\n\u001b[1;32m     13\u001b[0m #     \"\"\"\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m #     def __init__(self, input, split_index, output_dir, n_threads=8, n_iterations=100, test_size=0.2,\n\u001b[1;32m     16\u001b[0m #                  batch_size=64, epochs=10, lr=0.0001, weight_decay=1e-4, verbose=False):\n\u001b[1;32m     17\u001b[0m #         self._input = input\n\u001b[1;32m     18\u001b[0m #         self._split_index = split_index\n\u001b[1;32m     19\u001b[0m #         self._output_dir = output_dir\n\u001b[1;32m     20\u001b[0m #         self._n_threads = n_threads\n\u001b[1;32m     21\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m     22\u001b[0m #         self._batch_size = batch_size\n\u001b[1;32m     23\u001b[0m #         self._epochs = epochs\n\u001b[1;32m     24\u001b[0m #         self._lr = lr\n\u001b[1;32m     25\u001b[0m #         self._weight_decay = weight_decay\n\u001b[1;32m     26\u001b[0m #         self._verbose = verbose\n\u001b[1;32m     27\u001b[0m #         self._test_size = test_size\n\u001b[1;32m     28\u001b[0m #         self._validation = None\n\u001b[1;32m     29\u001b[0m #         self._algorithm = None\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m #     def run(self):\n\u001b[1;32m     32\u001b[0m #         x = self._input.get_x()\n\u001b[1;32m     33\u001b[0m #         y = self._input.get_y_raw()\n\u001b[1;32m     34\u001b[0m #         df_header = self._input.get_participant_session_id()\n\u001b[1;32m     35\u001b[0m #         input_dim = x.shape[1]\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m #         x_tensor = torch.FloatTensor(x)\n\u001b[1;32m     38\u001b[0m #         y_tensor = torch.FloatTensor(y)\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m #         self._algorithm = NNRegressionAlgorithm(\n\u001b[1;32m     41\u001b[0m #             x_tensor, y_tensor, self._output_dir, df_header, \n\u001b[1;32m     42\u001b[0m #             self._n_iterations,\n\u001b[1;32m     43\u001b[0m #             batch_size=self._batch_size,\n\u001b[1;32m     44\u001b[0m #             epochs=self._epochs,\n\u001b[1;32m     45\u001b[0m #             lr=self._lr,\n\u001b[1;32m     46\u001b[0m #             weight_decay=self._weight_decay, \n\u001b[1;32m     47\u001b[0m #             verbose=self._verbose\n\u001b[1;32m     48\u001b[0m #         )\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m #         self._validation = RepeatedHoldOut(\n\u001b[1;32m     51\u001b[0m #             self._algorithm, input_dim, \n\u001b[1;32m     52\u001b[0m #             n_iterations=self._n_iterations, \n\u001b[1;32m     53\u001b[0m #             test_size=self._test_size\n\u001b[1;32m     54\u001b[0m #         )\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m #         self._validation.validate(y, splits_indices=self._split_index, verbose=self._verbose)\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m # class NNRegressionAlgorithm(RegressionAlgorithm):\n\u001b[1;32m     59\u001b[0m #     '''\n\u001b[1;32m     60\u001b[0m #     NN regression.\n\u001b[1;32m     61\u001b[0m #     '''\n\u001b[1;32m     62\u001b[0m #     def __init__(self, x, y, output_dir, df_header, n_iterations=100, batch_size=64, \n\u001b[1;32m     63\u001b[0m #                  epochs=10, lr=0.0001, weight_decay=1e-4, verbose=False):\n\u001b[1;32m     64\u001b[0m #         self._x = x\n\u001b[1;32m     65\u001b[0m #         self._y = y\n\u001b[1;32m     66\u001b[0m #         self._output_dir = output_dir\n\u001b[1;32m     67\u001b[0m #         self._df_header = df_header\n\u001b[1;32m     68\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m     69\u001b[0m #         self._batch_size = batch_size\n\u001b[1;32m     70\u001b[0m #         self._epochs = epochs\n\u001b[1;32m     71\u001b[0m #         self._lr = lr\n\u001b[1;32m     72\u001b[0m #         self._weight_decay = weight_decay\n\u001b[1;32m     73\u001b[0m #         self._verbose = verbose\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m #     def _lauch_nn(self, input_dim, x_train, x_test, y_train, y_test, fi):\n\u001b[1;32m     76\u001b[0m #         model = neural_network_regression_3LinerLayers(input_dim)\n\u001b[1;32m     77\u001b[0m #         model.cpu()\n\u001b[1;32m     78\u001b[0m #         init_state = copy.deepcopy(model.state_dict())\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m #         # train the NN - passing optimizer as string name\n\u001b[0;32m---> 81\u001b[0m #         train_network(\n\u001b[1;32m     82\u001b[0m #             model, \n\u001b[1;32m     83\u001b[0m #             self._output_dir, \n\u001b[1;32m     84\u001b[0m #             fi, \n\u001b[1;32m     85\u001b[0m #             x_train, \n\u001b[1;32m     86\u001b[0m #             y_train, \n\u001b[1;32m     87\u001b[0m #             x_test, \n\u001b[1;32m     88\u001b[0m #             y_test, \n\u001b[1;32m     89\u001b[0m #             self._epochs,\n\u001b[1;32m     90\u001b[0m #             self._batch_size, \n\u001b[1;32m     91\u001b[0m #             init_state, \n\u001b[1;32m     92\u001b[0m #             self._df_header,\n\u001b[1;32m     93\u001b[0m #             gpu=False,\n\u001b[1;32m     94\u001b[0m #             lr=self._lr, \n\u001b[1;32m     95\u001b[0m #             weight_decay=self._weight_decay,\n\u001b[1;32m     96\u001b[0m #             opt='Adam'  # Pass optimizer name as string\n\u001b[1;32m     97\u001b[0m #         )\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m #     def evaluate(self, input_dim, train_index, test_index, fi):\n\u001b[1;32m    100\u001b[0m #         x_train = self._x[train_index]\n\u001b[1;32m    101\u001b[0m #         y_train = self._y[train_index]\n\u001b[1;32m    102\u001b[0m #         x_test = self._x[test_index]\n\u001b[1;32m    103\u001b[0m #         y_test = self._y[test_index]\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m #         results = self._lauch_nn(input_dim, x_train, x_test, y_train, y_test, fi)\n\u001b[1;32m    106\u001b[0m #         return results\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m # class RepeatedHoldOut(RegressionValidation):\n\u001b[1;32m    109\u001b[0m #     \"\"\"\n\u001b[1;32m    110\u001b[0m #     Repeated holdout splits CV.\n\u001b[1;32m    111\u001b[0m #     \"\"\"\n\u001b[1;32m    112\u001b[0m #     def __init__(self, ml_algorithm, input_dim, n_iterations=100, test_size=0.3):\n\u001b[1;32m    113\u001b[0m #         self._ml_algorithm = ml_algorithm\n\u001b[1;32m    114\u001b[0m #         self._input_dim = input_dim\n\u001b[1;32m    115\u001b[0m #         self._split_results = []\n\u001b[1;32m    116\u001b[0m #         self._cv = None\n\u001b[1;32m    117\u001b[0m #         self._n_iterations = n_iterations\n\u001b[1;32m    118\u001b[0m #         self._test_size = test_size\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m #     def validate(self, y, splits_indices=None, verbose=False):\n\u001b[1;32m    121\u001b[0m #         if splits_indices is None:\n\u001b[1;32m    122\u001b[0m #             splits = ShuffleSplit(n_splits=self._n_iterations, test_size=self._test_size)\n\u001b[1;32m    123\u001b[0m #             self._cv = list(splits.split(np.zeros(len(y)), y))\n\u001b[1;32m    124\u001b[0m #         else:\n\u001b[1;32m    125\u001b[0m #             self._cv = splits_indices\n\u001b[1;32m    126\u001b[0m #         results = {}\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m #         for i in range(self._n_iterations):\n\u001b[1;32m    129\u001b[0m #             time_bar(i, self._n_iterations)\n\u001b[1;32m    130\u001b[0m #             print()\n\u001b[1;32m    131\u001b[0m #             if verbose:\n\u001b[1;32m    132\u001b[0m #                 print(\"Repetition %d of CV...\" % i)\n\u001b[1;32m    133\u001b[0m #             train_index, test_index = self._cv[i]\n\u001b[1;32m    134\u001b[0m #             results[i] = self._ml_algorithm.evaluate(self._input_dim, train_index, test_index, i)\n\u001b[1;32m    136\u001b[0m from mlni.base import WorkFlow, RegressionAlgorithm, RegressionValidation\n\u001b[1;32m    137\u001b[0m import numpy as np\n",
      "File \u001b[0;32m~/Documents/Research/mlni/mlni/utils.py:1437\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(model, output_dir, fi, X_train, y_train, X_test, y_test, num_epochs, batch_size, init_state, df_header, gpu, lr, weight_decay, opt)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor training, mean MAE loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m during each epoch using batch data \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (mean_mae_train, epoch))\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;66;03m# calculate the accuracy with the whole training data for subject level balanced accuracy\u001b[39;00m\n\u001b[0;32m-> 1437\u001b[0m train_all_df, mean_mae_train_all, _ \u001b[38;5;241m=\u001b[39m train(model, train_loader, gpu, loss, optimizer, writer_train_all_data, epoch, model_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor training, mean MAE loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m at the end of epoch by applying all training data \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (mean_mae_train_all, epoch))\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;66;03m# at then end of each epoch, we validate one time for the model with the validation data\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/mlni/mlni/utils.py:1264\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, use_cuda, loss_func, optimizer, writer, epoch, model_mode)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1264\u001b[0m     results_df, total_loss \u001b[38;5;241m=\u001b[39m test(model, data_loader, use_cuda, loss_func)\n\u001b[1;32m   1265\u001b[0m     loss_batch_mean \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n\u001b[1;32m   1266\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_batch_mean, epoch)\n",
      "File \u001b[0;32m~/Documents/Research/mlni/mlni/utils.py:1293\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, data_loader, use_cuda, loss_func)\u001b[0m\n\u001b[1;32m   1291\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_cuda:\n\u001b[1;32m   1295\u001b[0m             imgs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda(), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:948\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    945\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mlni.adml_regression import regression_roi \n",
    "from mlni.adml_regression_rbf import regression_roi as regression_roi_rbf\n",
    "from mlni.adml_regression_nn import regression_roi as regression_roi_nn\n",
    "from mlni.adml_regression_mlp import regression_roi as regression_roi_mlp\n",
    "from mlni.adml_regression_lasso import regression_roi as regression_roi_lasso\n",
    "\n",
    "# Define paths\n",
    "final_datasets_directory = \"dataset/final_datasets\"\n",
    "output_base_directory = \"result_by_type\"\n",
    "cv_repetition = 250  # Number of cross-validation repetitions\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_base_directory, exist_ok=True)\n",
    "\n",
    "# Get a list of all TSV files in the final_datasets directory\n",
    "tsv_files = [f for f in os.listdir(final_datasets_directory) if f.endswith(\".tsv\")]\n",
    "\n",
    "# Define regression types and their corresponding functions\n",
    "regression_types = {\n",
    "    'svr_linear': regression_roi,\n",
    "    'svr_rbf': regression_roi_rbf,\n",
    "    'nn': regression_roi_nn,\n",
    "    'mlp': regression_roi_mlp,\n",
    "    'lasso': regression_roi_lasso\n",
    "}\n",
    "\n",
    "# Iterate through each TSV file\n",
    "for tsv_file in tsv_files:\n",
    "    feature_tsv_path = os.path.join(final_datasets_directory, tsv_file)\n",
    "    \n",
    "    # Extract just the organ system name\n",
    "    # If file is \"filtered_ADNI_brain.tsv\", this will give us \"brain\"\n",
    "    organ_system = tsv_file.split('_')[-1].replace('.tsv', '')\n",
    "    \n",
    "    # Run each type of regression\n",
    "    for reg_type, reg_function in regression_types.items():\n",
    "        try:\n",
    "            # Create output directory using organ system name and regression type\n",
    "            dataset_name = f\"{organ_system}_{reg_type}\"\n",
    "            output_dir = os.path.join(output_base_directory, dataset_name)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            print(f\"Running {reg_type} regression for {organ_system}...\")\n",
    "            # Run regression\n",
    "            reg_function(feature_tsv_path, output_dir, cv_repetition)\n",
    "            print(f\"{reg_type} regression completed for {organ_system}. Results saved in {output_dir}.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error running {reg_type} regression for {organ_system}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(\"All datasets processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlni.regression_analysis import run_regression_analysis\n",
    "\n",
    "# # Define paths\n",
    "# final_datasets_directory = \"dataset/final_datasets\"\n",
    "# output_base_directory = \"dataset/result_by_type\"\n",
    "# cv_repetition = 250\n",
    "\n",
    "# # Run the analysis\n",
    "# results = run_regression_analysis(final_datasets_directory, output_base_directory, cv_repetition)\n",
    "\n",
    "# # Display results\n",
    "# print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend available\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"MPS backend available\")\n",
    "# else:\n",
    "#     print(\"MPS backend not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(torch.cuda.is_available())  # Should print True\n",
    "# print(torch.version.cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
